{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KheG-DAyeS4",
        "outputId": "7055a0c9-e94b-4dba-d855-3e52df84aa40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMOgvb0zyeS7",
        "outputId": "8ce7c490-e858-4f49-df55-7c1198d0ad87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.18.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2025.1.31)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GZQIASiGyeS7"
      },
      "outputs": [],
      "source": [
        "! rm -rf video.mp4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "aJwZEe4myeS7",
        "outputId": "5f34c963-a2c6-48d6-b31a-9b9bab94c0a4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=15C3ZIbvy4CemGBOmIkrGK7H7a2tpYaBJ\n",
            "To: /content/people-counting.mp4\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 42.5M/42.5M [00:01<00:00, 38.0MB/s]\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'people-counting.mp4'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gdown\n",
        "\n",
        "# Update with your file's specific ID\n",
        "file_id = \"15C3ZIbvy4CemGBOmIkrGK7H7a2tpYaBJ\"\n",
        "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "\n",
        "output = \"people-counting.mp4\"\n",
        "gdown.download(url, output, quiet=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qB8rlAdEyeS8",
        "outputId": "6081b19d-bbcc-4eb0-e8bc-48781eee7400"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "HOME = os.getcwd()\n",
        "print(HOME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "y6itgqQiyeS8"
      },
      "outputs": [],
      "source": [
        "SOURCE_VIDEO_PATH = os.path.join(HOME, \"people-counting.mp4\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPnUh0tTyeS8",
        "outputId": "83c56aa2-5fa5-4ba5-9493-01c77d857e27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics 8.3.40 ðŸš€ Python-3.11.11 torch-2.6.0+cu124 CPU (Intel Xeon 2.20GHz)\n",
            "Setup complete âœ… (2 CPUs, 12.7 GB RAM, 41.2/107.7 GB disk)\n"
          ]
        }
      ],
      "source": [
        "# Pip install method (recommended)\n",
        "\n",
        "!pip install \"ultralytics<=8.3.40\"\n",
        "\n",
        "from IPython import display\n",
        "display.clear_output()\n",
        "\n",
        "import ultralytics\n",
        "ultralytics.checks()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XC4UIOlZyeS9",
        "outputId": "ea978bce-1913-415e-a5e2-140d136e8176"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "supervision.__version__: 0.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install supervision==0.3.0\n",
        "\n",
        "from IPython import display\n",
        "display.clear_output()\n",
        "\n",
        "import supervision\n",
        "print(\"supervision.__version__:\", supervision.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "UP8L8DEgyeS9"
      },
      "outputs": [],
      "source": [
        "# settings\n",
        "MODEL = \"yolov8x.pt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PDmE_WJyeS-",
        "outputId": "186bb942-b606-4628-a400-f867529a4f3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "YOLOv8x summary (fused): 268 layers, 68,200,608 parameters, 0 gradients, 257.8 GFLOPs\n"
          ]
        }
      ],
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO(MODEL)\n",
        "model.fuse()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmW3EbL2yeS-",
        "outputId": "ea962e22-aa76-4d47-bcc1-e3fec4ac7b63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8s.pt to 'yolov8s.pt'...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21.5M/21.5M [00:00<00:00, 41.2MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class ID: 0, Class Name: person\n",
            "Class ID: 1, Class Name: bicycle\n",
            "Class ID: 2, Class Name: car\n",
            "Class ID: 3, Class Name: motorcycle\n",
            "Class ID: 4, Class Name: airplane\n",
            "Class ID: 5, Class Name: bus\n",
            "Class ID: 6, Class Name: train\n",
            "Class ID: 7, Class Name: truck\n",
            "Class ID: 8, Class Name: boat\n",
            "Class ID: 9, Class Name: traffic light\n",
            "Class ID: 10, Class Name: fire hydrant\n",
            "Class ID: 11, Class Name: stop sign\n",
            "Class ID: 12, Class Name: parking meter\n",
            "Class ID: 13, Class Name: bench\n",
            "Class ID: 14, Class Name: bird\n",
            "Class ID: 15, Class Name: cat\n",
            "Class ID: 16, Class Name: dog\n",
            "Class ID: 17, Class Name: horse\n",
            "Class ID: 18, Class Name: sheep\n",
            "Class ID: 19, Class Name: cow\n",
            "Class ID: 20, Class Name: elephant\n",
            "Class ID: 21, Class Name: bear\n",
            "Class ID: 22, Class Name: zebra\n",
            "Class ID: 23, Class Name: giraffe\n",
            "Class ID: 24, Class Name: backpack\n",
            "Class ID: 25, Class Name: umbrella\n",
            "Class ID: 26, Class Name: handbag\n",
            "Class ID: 27, Class Name: tie\n",
            "Class ID: 28, Class Name: suitcase\n",
            "Class ID: 29, Class Name: frisbee\n",
            "Class ID: 30, Class Name: skis\n",
            "Class ID: 31, Class Name: snowboard\n",
            "Class ID: 32, Class Name: sports ball\n",
            "Class ID: 33, Class Name: kite\n",
            "Class ID: 34, Class Name: baseball bat\n",
            "Class ID: 35, Class Name: baseball glove\n",
            "Class ID: 36, Class Name: skateboard\n",
            "Class ID: 37, Class Name: surfboard\n",
            "Class ID: 38, Class Name: tennis racket\n",
            "Class ID: 39, Class Name: bottle\n",
            "Class ID: 40, Class Name: wine glass\n",
            "Class ID: 41, Class Name: cup\n",
            "Class ID: 42, Class Name: fork\n",
            "Class ID: 43, Class Name: knife\n",
            "Class ID: 44, Class Name: spoon\n",
            "Class ID: 45, Class Name: bowl\n",
            "Class ID: 46, Class Name: banana\n",
            "Class ID: 47, Class Name: apple\n",
            "Class ID: 48, Class Name: sandwich\n",
            "Class ID: 49, Class Name: orange\n",
            "Class ID: 50, Class Name: broccoli\n",
            "Class ID: 51, Class Name: carrot\n",
            "Class ID: 52, Class Name: hot dog\n",
            "Class ID: 53, Class Name: pizza\n",
            "Class ID: 54, Class Name: donut\n",
            "Class ID: 55, Class Name: cake\n",
            "Class ID: 56, Class Name: chair\n",
            "Class ID: 57, Class Name: couch\n",
            "Class ID: 58, Class Name: potted plant\n",
            "Class ID: 59, Class Name: bed\n",
            "Class ID: 60, Class Name: dining table\n",
            "Class ID: 61, Class Name: toilet\n",
            "Class ID: 62, Class Name: tv\n",
            "Class ID: 63, Class Name: laptop\n",
            "Class ID: 64, Class Name: mouse\n",
            "Class ID: 65, Class Name: remote\n",
            "Class ID: 66, Class Name: keyboard\n",
            "Class ID: 67, Class Name: cell phone\n",
            "Class ID: 68, Class Name: microwave\n",
            "Class ID: 69, Class Name: oven\n",
            "Class ID: 70, Class Name: toaster\n",
            "Class ID: 71, Class Name: sink\n",
            "Class ID: 72, Class Name: refrigerator\n",
            "Class ID: 73, Class Name: book\n",
            "Class ID: 74, Class Name: clock\n",
            "Class ID: 75, Class Name: vase\n",
            "Class ID: 76, Class Name: scissors\n",
            "Class ID: 77, Class Name: teddy bear\n",
            "Class ID: 78, Class Name: hair drier\n",
            "Class ID: 79, Class Name: toothbrush\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "# Load YOLO model\n",
        "model = YOLO('yolov8s.pt')\n",
        "\n",
        "# Get class names from the model\n",
        "class_names = model.names  # Dictionary {class_id: class_name}\n",
        "\n",
        "# Print all class IDs and names\n",
        "for class_id, class_name in class_names.items():\n",
        "    print(f\"Class ID: {class_id}, Class Name: {class_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4u6RIc6tGXu"
      },
      "source": [
        "#### Track and Count People \n",
        "\n",
        "* Count total people in the video\n",
        "* Count people entering and exiting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-IcgBOGtI4m",
        "outputId": "714f06fd-b76b-4ec2-d68a-80337bb179ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "video 1/1 (frame 1/592) /content/people-counting.mp4: 384x640 1 potted plant, 3863.9ms\n",
            "video 1/1 (frame 2/592) /content/people-counting.mp4: 384x640 1 car, 1 potted plant, 2814.4ms\n",
            "video 1/1 (frame 3/592) /content/people-counting.mp4: 384x640 1 car, 1 potted plant, 2830.6ms\n",
            "video 1/1 (frame 4/592) /content/people-counting.mp4: 384x640 1 car, 1 potted plant, 2758.0ms\n",
            "video 1/1 (frame 5/592) /content/people-counting.mp4: 384x640 1 car, 1 potted plant, 3898.0ms\n",
            "video 1/1 (frame 6/592) /content/people-counting.mp4: 384x640 1 person, 1 car, 1 potted plant, 2775.9ms\n",
            "video 1/1 (frame 7/592) /content/people-counting.mp4: 384x640 1 person, 1 car, 1 potted plant, 2756.7ms\n",
            "video 1/1 (frame 8/592) /content/people-counting.mp4: 384x640 1 person, 1 car, 1 potted plant, 2747.6ms\n",
            "video 1/1 (frame 9/592) /content/people-counting.mp4: 384x640 1 person, 1 car, 1 potted plant, 3954.7ms\n",
            "video 1/1 (frame 10/592) /content/people-counting.mp4: 384x640 1 person, 1 car, 1 bench, 1 potted plant, 2798.2ms\n",
            "video 1/1 (frame 11/592) /content/people-counting.mp4: 384x640 1 person, 1 car, 1 bench, 1 potted plant, 2765.5ms\n",
            "video 1/1 (frame 12/592) /content/people-counting.mp4: 384x640 1 person, 1 car, 1 bench, 1 potted plant, 2796.4ms\n",
            "video 1/1 (frame 13/592) /content/people-counting.mp4: 384x640 1 person, 1 car, 1 bench, 1 potted plant, 3863.2ms\n",
            "video 1/1 (frame 14/592) /content/people-counting.mp4: 384x640 1 person, 1 car, 1 bench, 1 potted plant, 2832.5ms\n",
            "video 1/1 (frame 15/592) /content/people-counting.mp4: 384x640 1 person, 1 car, 1 bench, 1 potted plant, 2790.1ms\n",
            "video 1/1 (frame 16/592) /content/people-counting.mp4: 384x640 1 person, 1 car, 1 bench, 1 potted plant, 2805.2ms\n",
            "video 1/1 (frame 17/592) /content/people-counting.mp4: 384x640 1 person, 1 car, 1 bench, 1 potted plant, 3697.5ms\n",
            "video 1/1 (frame 18/592) /content/people-counting.mp4: 384x640 1 person, 1 car, 1 bench, 1 potted plant, 3066.2ms\n",
            "video 1/1 (frame 19/592) /content/people-counting.mp4: 384x640 1 person, 1 car, 1 bench, 1 potted plant, 2747.7ms\n",
            "video 1/1 (frame 20/592) /content/people-counting.mp4: 384x640 1 person, 1 car, 1 bench, 1 potted plant, 2791.0ms\n",
            "video 1/1 (frame 21/592) /content/people-counting.mp4: 384x640 1 person, 1 car, 1 bench, 1 potted plant, 3434.1ms\n",
            "video 1/1 (frame 22/592) /content/people-counting.mp4: 384x640 1 person, 1 car, 1 bench, 1 potted plant, 3200.9ms\n",
            "video 1/1 (frame 23/592) /content/people-counting.mp4: 384x640 1 person, 1 car, 1 bench, 1 potted plant, 2733.7ms\n",
            "video 1/1 (frame 24/592) /content/people-counting.mp4: 384x640 1 person, 1 car, 1 bench, 1 potted plant, 2733.3ms\n",
            "video 1/1 (frame 25/592) /content/people-counting.mp4: 384x640 1 person, 1 car, 1 bench, 1 potted plant, 3193.2ms\n",
            "video 1/1 (frame 26/592) /content/people-counting.mp4: 384x640 1 person, 1 car, 1 bench, 1 potted plant, 3421.8ms\n",
            "video 1/1 (frame 27/592) /content/people-counting.mp4: 384x640 1 person, 1 car, 1 bench, 1 potted plant, 2782.5ms\n",
            "video 1/1 (frame 28/592) /content/people-counting.mp4: 384x640 1 person, 1 car, 1 bench, 1 potted plant, 2795.3ms\n",
            "video 1/1 (frame 29/592) /content/people-counting.mp4: 384x640 1 person, 1 car, 1 bench, 1 potted plant, 3567.3ms\n",
            "video 1/1 (frame 30/592) /content/people-counting.mp4: 384x640 1 person, 1 car, 1 bench, 1 potted plant, 3433.3ms\n",
            "video 1/1 (frame 31/592) /content/people-counting.mp4: 384x640 1 person, 1 car, 1 bench, 1 potted plant, 2827.3ms\n",
            "video 1/1 (frame 32/592) /content/people-counting.mp4: 384x640 1 person, 1 car, 1 bench, 1 potted plant, 2772.2ms\n",
            "video 1/1 (frame 33/592) /content/people-counting.mp4: 384x640 1 person, 1 car, 1 bench, 1 potted plant, 2883.4ms\n",
            "video 1/1 (frame 34/592) /content/people-counting.mp4: 384x640 1 person, 1 car, 1 bench, 1 potted plant, 3678.8ms\n",
            "video 1/1 (frame 35/592) /content/people-counting.mp4: 384x640 1 person, 1 car, 1 bench, 1 potted plant, 2758.1ms\n",
            "video 1/1 (frame 36/592) /content/people-counting.mp4: 384x640 1 person, 1 car, 1 bench, 2 potted plants, 2780.3ms\n",
            "video 1/1 (frame 37/592) /content/people-counting.mp4: 384x640 1 person, 1 car, 1 bench, 2 potted plants, 2780.4ms\n",
            "video 1/1 (frame 38/592) /content/people-counting.mp4: 384x640 1 person, 1 car, 1 bench, 2 potted plants, 3870.2ms\n",
            "video 1/1 (frame 39/592) /content/people-counting.mp4: 384x640 1 person, 1 car, 1 bench, 1 potted plant, 2768.8ms\n",
            "video 1/1 (frame 40/592) /content/people-counting.mp4: 384x640 2 persons, 1 car, 1 bench, 1 potted plant, 2789.9ms\n",
            "video 1/1 (frame 41/592) /content/people-counting.mp4: 384x640 2 persons, 1 car, 1 bench, 1 potted plant, 2786.0ms\n",
            "video 1/1 (frame 42/592) /content/people-counting.mp4: 384x640 2 persons, 1 car, 1 bench, 1 potted plant, 3890.5ms\n",
            "video 1/1 (frame 43/592) /content/people-counting.mp4: 384x640 2 persons, 1 car, 1 bench, 1 potted plant, 2751.8ms\n",
            "video 1/1 (frame 44/592) /content/people-counting.mp4: 384x640 3 persons, 1 car, 1 bench, 1 potted plant, 2779.7ms\n",
            "video 1/1 (frame 45/592) /content/people-counting.mp4: 384x640 3 persons, 2 cars, 1 bench, 2763.7ms\n",
            "video 1/1 (frame 46/592) /content/people-counting.mp4: 384x640 3 persons, 1 car, 1 bench, 3925.2ms\n",
            "video 1/1 (frame 47/592) /content/people-counting.mp4: 384x640 3 persons, 1 car, 1 bench, 2791.6ms\n",
            "video 1/1 (frame 48/592) /content/people-counting.mp4: 384x640 3 persons, 1 car, 1 bench, 2755.1ms\n",
            "video 1/1 (frame 49/592) /content/people-counting.mp4: 384x640 3 persons, 1 bench, 2776.9ms\n",
            "video 1/1 (frame 50/592) /content/people-counting.mp4: 384x640 3 persons, 1 bench, 3643.5ms\n",
            "video 1/1 (frame 51/592) /content/people-counting.mp4: 384x640 3 persons, 1 bench, 2978.4ms\n",
            "video 1/1 (frame 52/592) /content/people-counting.mp4: 384x640 3 persons, 1 bench, 2775.0ms\n",
            "video 1/1 (frame 53/592) /content/people-counting.mp4: 384x640 4 persons, 1 bench, 2738.7ms\n",
            "video 1/1 (frame 54/592) /content/people-counting.mp4: 384x640 4 persons, 1 bench, 3416.4ms\n",
            "video 1/1 (frame 55/592) /content/people-counting.mp4: 384x640 4 persons, 1 bench, 3224.5ms\n",
            "video 1/1 (frame 56/592) /content/people-counting.mp4: 384x640 4 persons, 1 bench, 1 potted plant, 2799.8ms\n",
            "video 1/1 (frame 57/592) /content/people-counting.mp4: 384x640 4 persons, 1 bench, 1 potted plant, 2815.8ms\n",
            "video 1/1 (frame 58/592) /content/people-counting.mp4: 384x640 4 persons, 1 car, 1 bench, 1 potted plant, 3256.4ms\n",
            "video 1/1 (frame 59/592) /content/people-counting.mp4: 384x640 3 persons, 1 bench, 1 potted plant, 3404.9ms\n",
            "video 1/1 (frame 60/592) /content/people-counting.mp4: 384x640 3 persons, 1 bench, 1 potted plant, 2748.5ms\n",
            "video 1/1 (frame 61/592) /content/people-counting.mp4: 384x640 3 persons, 1 bench, 1 potted plant, 2743.6ms\n",
            "video 1/1 (frame 62/592) /content/people-counting.mp4: 384x640 3 persons, 1 bench, 1 potted plant, 2924.9ms\n",
            "video 1/1 (frame 63/592) /content/people-counting.mp4: 384x640 3 persons, 1 bench, 1 potted plant, 4073.8ms\n",
            "video 1/1 (frame 64/592) /content/people-counting.mp4: 384x640 3 persons, 1 car, 1 bench, 1 potted plant, 2760.6ms\n",
            "video 1/1 (frame 65/592) /content/people-counting.mp4: 384x640 3 persons, 1 car, 1 bench, 1 potted plant, 2769.9ms\n",
            "video 1/1 (frame 66/592) /content/people-counting.mp4: 384x640 3 persons, 1 car, 1 bench, 1 potted plant, 3002.9ms\n",
            "video 1/1 (frame 67/592) /content/people-counting.mp4: 384x640 3 persons, 1 car, 1 bench, 1 potted plant, 3633.0ms\n",
            "video 1/1 (frame 68/592) /content/people-counting.mp4: 384x640 3 persons, 1 car, 1 bench, 1 potted plant, 2768.6ms\n",
            "video 1/1 (frame 69/592) /content/people-counting.mp4: 384x640 3 persons, 1 car, 1 bench, 1 potted plant, 2802.0ms\n",
            "video 1/1 (frame 70/592) /content/people-counting.mp4: 384x640 3 persons, 1 car, 1 bench, 1 potted plant, 2747.2ms\n",
            "video 1/1 (frame 71/592) /content/people-counting.mp4: 384x640 3 persons, 1 car, 1 bench, 1 potted plant, 3851.7ms\n",
            "video 1/1 (frame 72/592) /content/people-counting.mp4: 384x640 3 persons, 1 bench, 1 potted plant, 2727.7ms\n",
            "video 1/1 (frame 73/592) /content/people-counting.mp4: 384x640 3 persons, 1 bench, 1 potted plant, 2785.3ms\n",
            "video 1/1 (frame 74/592) /content/people-counting.mp4: 384x640 3 persons, 1 bench, 1 potted plant, 2719.9ms\n",
            "video 1/1 (frame 75/592) /content/people-counting.mp4: 384x640 3 persons, 1 bench, 2 potted plants, 3910.7ms\n",
            "video 1/1 (frame 76/592) /content/people-counting.mp4: 384x640 3 persons, 1 bench, 2 potted plants, 2726.1ms\n",
            "video 1/1 (frame 77/592) /content/people-counting.mp4: 384x640 3 persons, 1 bench, 1 potted plant, 2753.6ms\n",
            "video 1/1 (frame 78/592) /content/people-counting.mp4: 384x640 3 persons, 1 bench, 1 potted plant, 2727.5ms\n",
            "video 1/1 (frame 79/592) /content/people-counting.mp4: 384x640 3 persons, 1 bench, 1 potted plant, 3783.7ms\n",
            "video 1/1 (frame 80/592) /content/people-counting.mp4: 384x640 2 persons, 1 bench, 1 potted plant, 2924.3ms\n",
            "video 1/1 (frame 81/592) /content/people-counting.mp4: 384x640 2 persons, 1 bench, 1 potted plant, 2784.4ms\n",
            "video 1/1 (frame 82/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 2754.6ms\n",
            "video 1/1 (frame 83/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 3542.8ms\n",
            "video 1/1 (frame 84/592) /content/people-counting.mp4: 384x640 2 persons, 3014.9ms\n",
            "video 1/1 (frame 85/592) /content/people-counting.mp4: 384x640 2 persons, 2834.2ms\n",
            "video 1/1 (frame 86/592) /content/people-counting.mp4: 384x640 2 persons, 2772.4ms\n",
            "video 1/1 (frame 87/592) /content/people-counting.mp4: 384x640 2 persons, 3345.2ms\n",
            "video 1/1 (frame 88/592) /content/people-counting.mp4: 384x640 2 persons, 3402.1ms\n",
            "video 1/1 (frame 89/592) /content/people-counting.mp4: 384x640 2 persons, 2758.6ms\n",
            "video 1/1 (frame 90/592) /content/people-counting.mp4: 384x640 2 persons, 2788.5ms\n",
            "video 1/1 (frame 91/592) /content/people-counting.mp4: 384x640 2 persons, 3240.1ms\n",
            "video 1/1 (frame 92/592) /content/people-counting.mp4: 384x640 2 persons, 3483.6ms\n",
            "video 1/1 (frame 93/592) /content/people-counting.mp4: 384x640 2 persons, 2773.3ms\n",
            "video 1/1 (frame 94/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 2807.6ms\n",
            "video 1/1 (frame 95/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 3001.0ms\n",
            "video 1/1 (frame 96/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 3620.8ms\n",
            "video 1/1 (frame 97/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 2746.2ms\n",
            "video 1/1 (frame 98/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 2735.8ms\n",
            "video 1/1 (frame 99/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 2733.0ms\n",
            "video 1/1 (frame 100/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 3876.4ms\n",
            "video 1/1 (frame 101/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 2736.9ms\n",
            "video 1/1 (frame 102/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 2722.6ms\n",
            "video 1/1 (frame 103/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 2734.0ms\n",
            "video 1/1 (frame 104/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 3954.9ms\n",
            "video 1/1 (frame 105/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 2752.6ms\n",
            "video 1/1 (frame 106/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 2730.1ms\n",
            "video 1/1 (frame 107/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 2768.5ms\n",
            "video 1/1 (frame 108/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 3830.5ms\n",
            "video 1/1 (frame 109/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 2777.7ms\n",
            "video 1/1 (frame 110/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 2723.7ms\n",
            "video 1/1 (frame 111/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 2713.4ms\n",
            "video 1/1 (frame 112/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 3536.0ms\n",
            "video 1/1 (frame 113/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 3153.6ms\n",
            "video 1/1 (frame 114/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 2807.0ms\n",
            "video 1/1 (frame 115/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 2733.3ms\n",
            "video 1/1 (frame 116/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 3249.8ms\n",
            "video 1/1 (frame 117/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 3323.9ms\n",
            "video 1/1 (frame 118/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 2715.1ms\n",
            "video 1/1 (frame 119/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 2710.4ms\n",
            "video 1/1 (frame 120/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 2851.3ms\n",
            "video 1/1 (frame 121/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3679.0ms\n",
            "video 1/1 (frame 122/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2740.4ms\n",
            "video 1/1 (frame 123/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2754.0ms\n",
            "video 1/1 (frame 124/592) /content/people-counting.mp4: 384x640 1 person, 1 parking meter, 1 potted plant, 2789.4ms\n",
            "video 1/1 (frame 125/592) /content/people-counting.mp4: 384x640 1 person, 1 parking meter, 1 potted plant, 3942.5ms\n",
            "video 1/1 (frame 126/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2799.6ms\n",
            "video 1/1 (frame 127/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2755.7ms\n",
            "video 1/1 (frame 128/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2758.2ms\n",
            "video 1/1 (frame 129/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3899.5ms\n",
            "video 1/1 (frame 130/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2750.1ms\n",
            "video 1/1 (frame 131/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2772.0ms\n",
            "video 1/1 (frame 132/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2733.7ms\n",
            "video 1/1 (frame 133/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3873.9ms\n",
            "video 1/1 (frame 134/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2751.3ms\n",
            "video 1/1 (frame 135/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2726.8ms\n",
            "video 1/1 (frame 136/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2762.5ms\n",
            "video 1/1 (frame 137/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3513.6ms\n",
            "video 1/1 (frame 138/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3022.0ms\n",
            "video 1/1 (frame 139/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2762.9ms\n",
            "video 1/1 (frame 140/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2736.9ms\n",
            "video 1/1 (frame 141/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3222.6ms\n",
            "video 1/1 (frame 142/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3395.6ms\n",
            "video 1/1 (frame 143/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2906.9ms\n",
            "video 1/1 (frame 144/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2806.6ms\n",
            "video 1/1 (frame 145/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3104.1ms\n",
            "video 1/1 (frame 146/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3555.6ms\n",
            "video 1/1 (frame 147/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2762.2ms\n",
            "video 1/1 (frame 148/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2769.4ms\n",
            "video 1/1 (frame 149/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2780.4ms\n",
            "video 1/1 (frame 150/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3758.7ms\n",
            "video 1/1 (frame 151/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2769.7ms\n",
            "video 1/1 (frame 152/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2732.0ms\n",
            "video 1/1 (frame 153/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2764.7ms\n",
            "video 1/1 (frame 154/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3912.9ms\n",
            "video 1/1 (frame 155/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2784.8ms\n",
            "video 1/1 (frame 156/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2742.3ms\n",
            "video 1/1 (frame 157/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2721.6ms\n",
            "video 1/1 (frame 158/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3887.8ms\n",
            "video 1/1 (frame 159/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2746.6ms\n",
            "video 1/1 (frame 160/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2737.4ms\n",
            "video 1/1 (frame 161/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2736.6ms\n",
            "video 1/1 (frame 162/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3589.3ms\n",
            "video 1/1 (frame 163/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2975.1ms\n",
            "video 1/1 (frame 164/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2758.7ms\n",
            "video 1/1 (frame 165/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2751.1ms\n",
            "video 1/1 (frame 166/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3288.1ms\n",
            "video 1/1 (frame 167/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3303.4ms\n",
            "video 1/1 (frame 168/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2747.4ms\n",
            "video 1/1 (frame 169/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2734.0ms\n",
            "video 1/1 (frame 170/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2980.7ms\n",
            "video 1/1 (frame 171/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3616.8ms\n",
            "video 1/1 (frame 172/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2821.3ms\n",
            "video 1/1 (frame 173/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2750.1ms\n",
            "video 1/1 (frame 174/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2744.5ms\n",
            "video 1/1 (frame 175/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3869.0ms\n",
            "video 1/1 (frame 176/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2736.5ms\n",
            "video 1/1 (frame 177/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2729.1ms\n",
            "video 1/1 (frame 178/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2797.0ms\n",
            "video 1/1 (frame 179/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3888.1ms\n",
            "video 1/1 (frame 180/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2760.7ms\n",
            "video 1/1 (frame 181/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2711.7ms\n",
            "video 1/1 (frame 182/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2775.8ms\n",
            "video 1/1 (frame 183/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3886.1ms\n",
            "video 1/1 (frame 184/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2795.0ms\n",
            "video 1/1 (frame 185/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2784.7ms\n",
            "video 1/1 (frame 186/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2767.9ms\n",
            "video 1/1 (frame 187/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3610.5ms\n",
            "video 1/1 (frame 188/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2943.4ms\n",
            "video 1/1 (frame 189/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2761.1ms\n",
            "video 1/1 (frame 190/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2767.9ms\n",
            "video 1/1 (frame 191/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3356.3ms\n",
            "video 1/1 (frame 192/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3294.6ms\n",
            "video 1/1 (frame 193/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2806.1ms\n",
            "video 1/1 (frame 194/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2764.8ms\n",
            "video 1/1 (frame 195/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3073.9ms\n",
            "video 1/1 (frame 196/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3500.8ms\n",
            "video 1/1 (frame 197/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2750.4ms\n",
            "video 1/1 (frame 198/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2735.7ms\n",
            "video 1/1 (frame 199/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2791.0ms\n",
            "video 1/1 (frame 200/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3879.5ms\n",
            "video 1/1 (frame 201/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2801.8ms\n",
            "video 1/1 (frame 202/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2750.0ms\n",
            "video 1/1 (frame 203/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2781.3ms\n",
            "video 1/1 (frame 204/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3913.8ms\n",
            "video 1/1 (frame 205/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2745.4ms\n",
            "video 1/1 (frame 206/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2753.9ms\n",
            "video 1/1 (frame 207/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2747.1ms\n",
            "video 1/1 (frame 208/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3936.2ms\n",
            "video 1/1 (frame 209/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2790.6ms\n",
            "video 1/1 (frame 210/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2758.9ms\n",
            "video 1/1 (frame 211/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2804.4ms\n",
            "video 1/1 (frame 212/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3790.1ms\n",
            "video 1/1 (frame 213/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2880.8ms\n",
            "video 1/1 (frame 214/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2771.4ms\n",
            "video 1/1 (frame 215/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2755.0ms\n",
            "video 1/1 (frame 216/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3413.2ms\n",
            "video 1/1 (frame 217/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3194.3ms\n",
            "video 1/1 (frame 218/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2805.6ms\n",
            "video 1/1 (frame 219/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2777.4ms\n",
            "video 1/1 (frame 220/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3346.2ms\n",
            "video 1/1 (frame 221/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3453.9ms\n",
            "video 1/1 (frame 222/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2839.6ms\n",
            "video 1/1 (frame 223/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2769.3ms\n",
            "video 1/1 (frame 224/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2984.9ms\n",
            "video 1/1 (frame 225/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3591.4ms\n",
            "video 1/1 (frame 226/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2763.1ms\n",
            "video 1/1 (frame 227/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2796.6ms\n",
            "video 1/1 (frame 228/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2783.4ms\n",
            "video 1/1 (frame 229/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3855.2ms\n",
            "video 1/1 (frame 230/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2800.6ms\n",
            "video 1/1 (frame 231/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2763.8ms\n",
            "video 1/1 (frame 232/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2759.2ms\n",
            "video 1/1 (frame 233/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3919.3ms\n",
            "video 1/1 (frame 234/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2729.5ms\n",
            "video 1/1 (frame 235/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2755.0ms\n",
            "video 1/1 (frame 236/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2738.0ms\n",
            "video 1/1 (frame 237/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3865.9ms\n",
            "video 1/1 (frame 238/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2789.3ms\n",
            "video 1/1 (frame 239/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2769.3ms\n",
            "video 1/1 (frame 240/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 2791.4ms\n",
            "video 1/1 (frame 241/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 3590.1ms\n",
            "video 1/1 (frame 242/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 3017.1ms\n",
            "video 1/1 (frame 243/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 2736.3ms\n",
            "video 1/1 (frame 244/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 2752.6ms\n",
            "video 1/1 (frame 245/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3183.4ms\n",
            "video 1/1 (frame 246/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 3360.7ms\n",
            "video 1/1 (frame 247/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 2776.0ms\n",
            "video 1/1 (frame 248/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 2754.4ms\n",
            "video 1/1 (frame 249/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 2968.7ms\n",
            "video 1/1 (frame 250/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 3703.2ms\n",
            "video 1/1 (frame 251/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 2823.4ms\n",
            "video 1/1 (frame 252/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 2769.9ms\n",
            "video 1/1 (frame 253/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 2756.9ms\n",
            "video 1/1 (frame 254/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 3899.8ms\n",
            "video 1/1 (frame 255/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 2751.0ms\n",
            "video 1/1 (frame 256/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 2738.6ms\n",
            "video 1/1 (frame 257/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 2809.3ms\n",
            "video 1/1 (frame 258/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 3905.4ms\n",
            "video 1/1 (frame 259/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 2850.8ms\n",
            "video 1/1 (frame 260/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 2767.5ms\n",
            "video 1/1 (frame 261/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 2819.4ms\n",
            "video 1/1 (frame 262/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 4121.1ms\n",
            "video 1/1 (frame 263/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 3691.5ms\n",
            "video 1/1 (frame 264/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 2762.5ms\n",
            "video 1/1 (frame 265/592) /content/people-counting.mp4: 384x640 3 persons, 1 potted plant, 2726.1ms\n",
            "video 1/1 (frame 266/592) /content/people-counting.mp4: 384x640 1 person, 1 car, 1 potted plant, 3964.3ms\n",
            "video 1/1 (frame 267/592) /content/people-counting.mp4: 384x640 2 persons, 1 car, 1 potted plant, 2732.6ms\n",
            "video 1/1 (frame 268/592) /content/people-counting.mp4: 384x640 2 persons, 1 car, 1 potted plant, 2827.4ms\n",
            "video 1/1 (frame 269/592) /content/people-counting.mp4: 384x640 3 persons, 1 car, 1 potted plant, 2745.9ms\n",
            "video 1/1 (frame 270/592) /content/people-counting.mp4: 384x640 3 persons, 1 car, 1 potted plant, 3958.8ms\n",
            "video 1/1 (frame 271/592) /content/people-counting.mp4: 384x640 3 persons, 1 car, 1 potted plant, 2741.1ms\n",
            "video 1/1 (frame 272/592) /content/people-counting.mp4: 384x640 3 persons, 1 potted plant, 2751.0ms\n",
            "video 1/1 (frame 273/592) /content/people-counting.mp4: 384x640 3 persons, 1 potted plant, 1 clock, 2756.7ms\n",
            "video 1/1 (frame 274/592) /content/people-counting.mp4: 384x640 3 persons, 1 potted plant, 1 clock, 3685.5ms\n",
            "video 1/1 (frame 275/592) /content/people-counting.mp4: 384x640 3 persons, 1 potted plant, 1 clock, 2903.3ms\n",
            "video 1/1 (frame 276/592) /content/people-counting.mp4: 384x640 3 persons, 1 potted plant, 1 clock, 2756.4ms\n",
            "video 1/1 (frame 277/592) /content/people-counting.mp4: 384x640 3 persons, 1 car, 1 potted plant, 1 clock, 2763.6ms\n",
            "video 1/1 (frame 278/592) /content/people-counting.mp4: 384x640 3 persons, 1 car, 1 potted plant, 1 clock, 3490.6ms\n",
            "video 1/1 (frame 279/592) /content/people-counting.mp4: 384x640 3 persons, 1 car, 1 potted plant, 1 clock, 3171.6ms\n",
            "video 1/1 (frame 280/592) /content/people-counting.mp4: 384x640 2 persons, 1 car, 1 potted plant, 1 clock, 2786.6ms\n",
            "video 1/1 (frame 281/592) /content/people-counting.mp4: 384x640 2 persons, 1 car, 1 potted plant, 2847.3ms\n",
            "video 1/1 (frame 282/592) /content/people-counting.mp4: 384x640 2 persons, 1 car, 1 potted plant, 1 clock, 3269.4ms\n",
            "video 1/1 (frame 283/592) /content/people-counting.mp4: 384x640 2 persons, 1 car, 1 potted plant, 1 clock, 3335.1ms\n",
            "video 1/1 (frame 284/592) /content/people-counting.mp4: 384x640 2 persons, 1 car, 1 potted plant, 1 clock, 2776.4ms\n",
            "video 1/1 (frame 285/592) /content/people-counting.mp4: 384x640 2 persons, 1 car, 1 potted plant, 1 clock, 2841.1ms\n",
            "video 1/1 (frame 286/592) /content/people-counting.mp4: 384x640 3 persons, 1 car, 1 potted plant, 1 clock, 3030.7ms\n",
            "video 1/1 (frame 287/592) /content/people-counting.mp4: 384x640 3 persons, 1 car, 1 potted plant, 3606.6ms\n",
            "video 1/1 (frame 288/592) /content/people-counting.mp4: 384x640 2 persons, 1 car, 1 potted plant, 2788.6ms\n",
            "video 1/1 (frame 289/592) /content/people-counting.mp4: 384x640 2 persons, 1 car, 1 potted plant, 2752.7ms\n",
            "video 1/1 (frame 290/592) /content/people-counting.mp4: 384x640 2 persons, 1 car, 1 potted plant, 2826.3ms\n",
            "video 1/1 (frame 291/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 3809.7ms\n",
            "video 1/1 (frame 292/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 2761.8ms\n",
            "video 1/1 (frame 293/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 2741.5ms\n",
            "video 1/1 (frame 294/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 2786.3ms\n",
            "video 1/1 (frame 295/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 3918.4ms\n",
            "video 1/1 (frame 296/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 2769.0ms\n",
            "video 1/1 (frame 297/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 2786.5ms\n",
            "video 1/1 (frame 298/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 2793.4ms\n",
            "video 1/1 (frame 299/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 3920.9ms\n",
            "video 1/1 (frame 300/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 2764.7ms\n",
            "video 1/1 (frame 301/592) /content/people-counting.mp4: 384x640 2 persons, 1 car, 1 potted plant, 2736.7ms\n",
            "video 1/1 (frame 302/592) /content/people-counting.mp4: 384x640 3 persons, 1 car, 1 potted plant, 2757.4ms\n",
            "video 1/1 (frame 303/592) /content/people-counting.mp4: 384x640 3 persons, 1 car, 1 potted plant, 3840.9ms\n",
            "video 1/1 (frame 304/592) /content/people-counting.mp4: 384x640 3 persons, 1 car, 1 potted plant, 2769.5ms\n",
            "video 1/1 (frame 305/592) /content/people-counting.mp4: 384x640 3 persons, 2 cars, 1 potted plant, 2761.1ms\n",
            "video 1/1 (frame 306/592) /content/people-counting.mp4: 384x640 3 persons, 2 cars, 1 potted plant, 2730.4ms\n",
            "video 1/1 (frame 307/592) /content/people-counting.mp4: 384x640 3 persons, 2 cars, 1 potted plant, 3517.2ms\n",
            "video 1/1 (frame 308/592) /content/people-counting.mp4: 384x640 3 persons, 2 cars, 1 potted plant, 3087.2ms\n",
            "video 1/1 (frame 309/592) /content/people-counting.mp4: 384x640 2 persons, 1 car, 1 potted plant, 2746.3ms\n",
            "video 1/1 (frame 310/592) /content/people-counting.mp4: 384x640 2 persons, 1 car, 1 handbag, 1 potted plant, 2722.1ms\n",
            "video 1/1 (frame 311/592) /content/people-counting.mp4: 384x640 2 persons, 1 car, 1 handbag, 1 potted plant, 3148.7ms\n",
            "video 1/1 (frame 312/592) /content/people-counting.mp4: 384x640 2 persons, 1 car, 1 handbag, 1 potted plant, 3379.6ms\n",
            "video 1/1 (frame 313/592) /content/people-counting.mp4: 384x640 2 persons, 1 car, 1 handbag, 1 potted plant, 2744.0ms\n",
            "video 1/1 (frame 314/592) /content/people-counting.mp4: 384x640 2 persons, 1 car, 1 handbag, 1 potted plant, 2735.4ms\n",
            "video 1/1 (frame 315/592) /content/people-counting.mp4: 384x640 2 persons, 1 car, 1 handbag, 1 potted plant, 2821.8ms\n",
            "video 1/1 (frame 316/592) /content/people-counting.mp4: 384x640 2 persons, 1 car, 1 handbag, 1 potted plant, 3777.7ms\n",
            "video 1/1 (frame 317/592) /content/people-counting.mp4: 384x640 2 persons, 1 car, 1 handbag, 1 potted plant, 2786.7ms\n",
            "video 1/1 (frame 318/592) /content/people-counting.mp4: 384x640 2 persons, 1 car, 1 handbag, 1 potted plant, 2735.5ms\n",
            "video 1/1 (frame 319/592) /content/people-counting.mp4: 384x640 1 person, 1 car, 1 handbag, 1 potted plant, 2738.3ms\n",
            "video 1/1 (frame 320/592) /content/people-counting.mp4: 384x640 1 person, 1 car, 1 handbag, 1 potted plant, 3859.4ms\n",
            "video 1/1 (frame 321/592) /content/people-counting.mp4: 384x640 1 person, 1 car, 1 handbag, 1 potted plant, 2753.0ms\n",
            "video 1/1 (frame 322/592) /content/people-counting.mp4: 384x640 2 persons, 1 car, 1 handbag, 1 potted plant, 2732.8ms\n",
            "video 1/1 (frame 323/592) /content/people-counting.mp4: 384x640 2 persons, 1 car, 1 handbag, 1 potted plant, 2763.7ms\n",
            "video 1/1 (frame 324/592) /content/people-counting.mp4: 384x640 2 persons, 1 car, 1 handbag, 1 potted plant, 3886.3ms\n",
            "video 1/1 (frame 325/592) /content/people-counting.mp4: 384x640 2 persons, 1 car, 1 handbag, 1 potted plant, 2759.2ms\n",
            "video 1/1 (frame 326/592) /content/people-counting.mp4: 384x640 2 persons, 1 car, 1 handbag, 1 potted plant, 2769.0ms\n",
            "video 1/1 (frame 327/592) /content/people-counting.mp4: 384x640 2 persons, 1 car, 1 handbag, 1 potted plant, 2757.0ms\n",
            "video 1/1 (frame 328/592) /content/people-counting.mp4: 384x640 2 persons, 1 car, 1 handbag, 1 potted plant, 3661.6ms\n",
            "video 1/1 (frame 329/592) /content/people-counting.mp4: 384x640 2 persons, 1 car, 1 handbag, 1 potted plant, 2893.6ms\n",
            "video 1/1 (frame 330/592) /content/people-counting.mp4: 384x640 2 persons, 1 car, 1 handbag, 1 potted plant, 2743.2ms\n",
            "video 1/1 (frame 331/592) /content/people-counting.mp4: 384x640 2 persons, 2 cars, 1 handbag, 1 potted plant, 2733.8ms\n",
            "video 1/1 (frame 332/592) /content/people-counting.mp4: 384x640 2 persons, 2 cars, 1 handbag, 1 potted plant, 3333.8ms\n",
            "video 1/1 (frame 333/592) /content/people-counting.mp4: 384x640 2 persons, 2 cars, 1 handbag, 1 potted plant, 3201.8ms\n",
            "video 1/1 (frame 334/592) /content/people-counting.mp4: 384x640 2 persons, 2 cars, 1 handbag, 1 potted plant, 2741.2ms\n",
            "video 1/1 (frame 335/592) /content/people-counting.mp4: 384x640 2 persons, 2 cars, 1 handbag, 1 potted plant, 2734.9ms\n",
            "video 1/1 (frame 336/592) /content/people-counting.mp4: 384x640 2 persons, 2 cars, 1 handbag, 1 potted plant, 3137.4ms\n",
            "video 1/1 (frame 337/592) /content/people-counting.mp4: 384x640 2 persons, 2 cars, 1 handbag, 1 potted plant, 3502.6ms\n",
            "video 1/1 (frame 338/592) /content/people-counting.mp4: 384x640 2 persons, 2 cars, 1 handbag, 1 potted plant, 2747.9ms\n",
            "video 1/1 (frame 339/592) /content/people-counting.mp4: 384x640 2 persons, 2 cars, 1 handbag, 1 potted plant, 2747.9ms\n",
            "video 1/1 (frame 340/592) /content/people-counting.mp4: 384x640 2 persons, 2 cars, 1 handbag, 1 potted plant, 2794.9ms\n",
            "video 1/1 (frame 341/592) /content/people-counting.mp4: 384x640 2 persons, 2 cars, 1 handbag, 1 potted plant, 3860.7ms\n",
            "video 1/1 (frame 342/592) /content/people-counting.mp4: 384x640 2 persons, 2 cars, 1 handbag, 1 potted plant, 2820.7ms\n",
            "video 1/1 (frame 343/592) /content/people-counting.mp4: 384x640 2 persons, 2 cars, 1 handbag, 1 potted plant, 2730.0ms\n",
            "video 1/1 (frame 344/592) /content/people-counting.mp4: 384x640 2 persons, 2 cars, 1 handbag, 2743.0ms\n",
            "video 1/1 (frame 345/592) /content/people-counting.mp4: 384x640 2 persons, 1 car, 1 handbag, 1 potted plant, 3921.2ms\n",
            "video 1/1 (frame 346/592) /content/people-counting.mp4: 384x640 2 persons, 1 car, 1 handbag, 1 potted plant, 2798.3ms\n",
            "video 1/1 (frame 347/592) /content/people-counting.mp4: 384x640 2 persons, 1 bench, 1 handbag, 1 potted plant, 2751.5ms\n",
            "video 1/1 (frame 348/592) /content/people-counting.mp4: 384x640 2 persons, 1 bench, 1 handbag, 1 potted plant, 2798.0ms\n",
            "video 1/1 (frame 349/592) /content/people-counting.mp4: 384x640 2 persons, 1 bench, 1 handbag, 3898.1ms\n",
            "video 1/1 (frame 350/592) /content/people-counting.mp4: 384x640 3 persons, 1 bench, 1 handbag, 2820.0ms\n",
            "video 1/1 (frame 351/592) /content/people-counting.mp4: 384x640 3 persons, 1 bench, 2783.0ms\n",
            "video 1/1 (frame 352/592) /content/people-counting.mp4: 384x640 3 persons, 1 bench, 2770.1ms\n",
            "video 1/1 (frame 353/592) /content/people-counting.mp4: 384x640 3 persons, 1 bench, 1 handbag, 3889.1ms\n",
            "video 1/1 (frame 354/592) /content/people-counting.mp4: 384x640 2 persons, 1 bench, 1 handbag, 2777.8ms\n",
            "video 1/1 (frame 355/592) /content/people-counting.mp4: 384x640 2 persons, 1 bench, 1 handbag, 1 potted plant, 2869.2ms\n",
            "video 1/1 (frame 356/592) /content/people-counting.mp4: 384x640 2 persons, 1 bench, 1 handbag, 1 potted plant, 1 clock, 2797.0ms\n",
            "video 1/1 (frame 357/592) /content/people-counting.mp4: 384x640 2 persons, 1 bench, 1 handbag, 1 potted plant, 1 clock, 3723.8ms\n",
            "video 1/1 (frame 358/592) /content/people-counting.mp4: 384x640 2 persons, 1 bench, 1 handbag, 1 potted plant, 1 clock, 2978.1ms\n",
            "video 1/1 (frame 359/592) /content/people-counting.mp4: 384x640 2 persons, 1 bench, 1 handbag, 1 potted plant, 1 clock, 2732.9ms\n",
            "video 1/1 (frame 360/592) /content/people-counting.mp4: 384x640 2 persons, 1 bench, 1 handbag, 1 potted plant, 1 clock, 2738.5ms\n",
            "video 1/1 (frame 361/592) /content/people-counting.mp4: 384x640 2 persons, 1 bench, 1 handbag, 1 potted plant, 3410.4ms\n",
            "video 1/1 (frame 362/592) /content/people-counting.mp4: 384x640 2 persons, 1 bench, 1 handbag, 1 potted plant, 3169.5ms\n",
            "video 1/1 (frame 363/592) /content/people-counting.mp4: 384x640 2 persons, 1 bench, 1 handbag, 2753.9ms\n",
            "video 1/1 (frame 364/592) /content/people-counting.mp4: 384x640 2 persons, 1 bench, 1 handbag, 1 potted plant, 2736.9ms\n",
            "video 1/1 (frame 365/592) /content/people-counting.mp4: 384x640 2 persons, 1 bench, 1 handbag, 1 potted plant, 3207.0ms\n",
            "video 1/1 (frame 366/592) /content/people-counting.mp4: 384x640 2 persons, 1 bench, 1 handbag, 1 potted plant, 3420.8ms\n",
            "video 1/1 (frame 367/592) /content/people-counting.mp4: 384x640 2 persons, 1 bench, 1 handbag, 1 potted plant, 2751.8ms\n",
            "video 1/1 (frame 368/592) /content/people-counting.mp4: 384x640 2 persons, 1 bench, 1 handbag, 1 potted plant, 2728.1ms\n",
            "video 1/1 (frame 369/592) /content/people-counting.mp4: 384x640 2 persons, 1 bench, 1 handbag, 1 potted plant, 2857.0ms\n",
            "video 1/1 (frame 370/592) /content/people-counting.mp4: 384x640 2 persons, 1 bench, 1 handbag, 1 potted plant, 3726.9ms\n",
            "video 1/1 (frame 371/592) /content/people-counting.mp4: 384x640 2 persons, 1 handbag, 1 potted plant, 1 clock, 2755.1ms\n",
            "video 1/1 (frame 372/592) /content/people-counting.mp4: 384x640 2 persons, 1 handbag, 1 potted plant, 1 clock, 2729.7ms\n",
            "video 1/1 (frame 373/592) /content/people-counting.mp4: 384x640 2 persons, 1 handbag, 1 potted plant, 1 clock, 2734.2ms\n",
            "video 1/1 (frame 374/592) /content/people-counting.mp4: 384x640 2 persons, 1 handbag, 1 potted plant, 1 clock, 3914.2ms\n",
            "video 1/1 (frame 375/592) /content/people-counting.mp4: 384x640 2 persons, 1 handbag, 1 potted plant, 2778.1ms\n",
            "video 1/1 (frame 376/592) /content/people-counting.mp4: 384x640 2 persons, 1 handbag, 1 potted plant, 2746.1ms\n",
            "video 1/1 (frame 377/592) /content/people-counting.mp4: 384x640 2 persons, 1 handbag, 1 potted plant, 2730.9ms\n",
            "video 1/1 (frame 378/592) /content/people-counting.mp4: 384x640 2 persons, 1 handbag, 1 potted plant, 3883.1ms\n",
            "video 1/1 (frame 379/592) /content/people-counting.mp4: 384x640 2 persons, 1 handbag, 1 potted plant, 2743.0ms\n",
            "video 1/1 (frame 380/592) /content/people-counting.mp4: 384x640 2 persons, 1 handbag, 1 potted plant, 2797.5ms\n",
            "video 1/1 (frame 381/592) /content/people-counting.mp4: 384x640 2 persons, 1 handbag, 1 potted plant, 2782.5ms\n",
            "video 1/1 (frame 382/592) /content/people-counting.mp4: 384x640 2 persons, 1 handbag, 1 potted plant, 3824.2ms\n",
            "video 1/1 (frame 383/592) /content/people-counting.mp4: 384x640 2 persons, 1 handbag, 1 potted plant, 2754.1ms\n",
            "video 1/1 (frame 384/592) /content/people-counting.mp4: 384x640 2 persons, 1 handbag, 1 potted plant, 2830.1ms\n",
            "video 1/1 (frame 385/592) /content/people-counting.mp4: 384x640 2 persons, 1 handbag, 1 potted plant, 2754.2ms\n",
            "video 1/1 (frame 386/592) /content/people-counting.mp4: 384x640 2 persons, 1 handbag, 1 potted plant, 3520.9ms\n",
            "video 1/1 (frame 387/592) /content/people-counting.mp4: 384x640 2 persons, 1 handbag, 3015.1ms\n",
            "video 1/1 (frame 388/592) /content/people-counting.mp4: 384x640 2 persons, 1 handbag, 2750.9ms\n",
            "video 1/1 (frame 389/592) /content/people-counting.mp4: 384x640 2 persons, 1 handbag, 2743.1ms\n",
            "video 1/1 (frame 390/592) /content/people-counting.mp4: 384x640 2 persons, 1 handbag, 3223.8ms\n",
            "video 1/1 (frame 391/592) /content/people-counting.mp4: 384x640 2 persons, 1 handbag, 3279.4ms\n",
            "video 1/1 (frame 392/592) /content/people-counting.mp4: 384x640 2 persons, 1 handbag, 2765.5ms\n",
            "video 1/1 (frame 393/592) /content/people-counting.mp4: 384x640 2 persons, 1 handbag, 2751.4ms\n",
            "video 1/1 (frame 394/592) /content/people-counting.mp4: 384x640 2 persons, 1 handbag, 3077.4ms\n",
            "video 1/1 (frame 395/592) /content/people-counting.mp4: 384x640 2 persons, 1 handbag, 3552.2ms\n",
            "video 1/1 (frame 396/592) /content/people-counting.mp4: 384x640 2 persons, 1 handbag, 1 potted plant, 2763.5ms\n",
            "video 1/1 (frame 397/592) /content/people-counting.mp4: 384x640 2 persons, 1 handbag, 2751.0ms\n",
            "video 1/1 (frame 398/592) /content/people-counting.mp4: 384x640 2 persons, 1 handbag, 2742.7ms\n",
            "video 1/1 (frame 399/592) /content/people-counting.mp4: 384x640 2 persons, 1 handbag, 3862.6ms\n",
            "video 1/1 (frame 400/592) /content/people-counting.mp4: 384x640 2 persons, 1 handbag, 2770.8ms\n",
            "video 1/1 (frame 401/592) /content/people-counting.mp4: 384x640 2 persons, 1 handbag, 2790.1ms\n",
            "video 1/1 (frame 402/592) /content/people-counting.mp4: 384x640 2 persons, 1 handbag, 2729.4ms\n",
            "video 1/1 (frame 403/592) /content/people-counting.mp4: 384x640 2 persons, 1 handbag, 3960.3ms\n",
            "video 1/1 (frame 404/592) /content/people-counting.mp4: 384x640 2 persons, 1 handbag, 2740.2ms\n",
            "video 1/1 (frame 405/592) /content/people-counting.mp4: 384x640 2 persons, 1 handbag, 2751.3ms\n",
            "video 1/1 (frame 406/592) /content/people-counting.mp4: 384x640 2 persons, 1 handbag, 2745.8ms\n",
            "video 1/1 (frame 407/592) /content/people-counting.mp4: 384x640 1 person, 1 handbag, 3889.8ms\n",
            "video 1/1 (frame 408/592) /content/people-counting.mp4: 384x640 1 person, 1 handbag, 2769.5ms\n",
            "video 1/1 (frame 409/592) /content/people-counting.mp4: 384x640 1 person, 1 handbag, 2777.1ms\n",
            "video 1/1 (frame 410/592) /content/people-counting.mp4: 384x640 1 person, 1 handbag, 2733.2ms\n",
            "video 1/1 (frame 411/592) /content/people-counting.mp4: 384x640 1 person, 1 handbag, 3662.2ms\n",
            "video 1/1 (frame 412/592) /content/people-counting.mp4: 384x640 1 person, 1 handbag, 2941.2ms\n",
            "video 1/1 (frame 413/592) /content/people-counting.mp4: 384x640 1 person, 1 handbag, 1 potted plant, 2841.1ms\n",
            "video 1/1 (frame 414/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2799.1ms\n",
            "video 1/1 (frame 415/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3388.9ms\n",
            "video 1/1 (frame 416/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3139.6ms\n",
            "video 1/1 (frame 417/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2752.2ms\n",
            "video 1/1 (frame 418/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2752.6ms\n",
            "video 1/1 (frame 419/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3146.1ms\n",
            "video 1/1 (frame 420/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3483.3ms\n",
            "video 1/1 (frame 421/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2771.0ms\n",
            "video 1/1 (frame 422/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2768.8ms\n",
            "video 1/1 (frame 423/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2832.4ms\n",
            "video 1/1 (frame 424/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3714.5ms\n",
            "video 1/1 (frame 425/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2724.4ms\n",
            "video 1/1 (frame 426/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2771.4ms\n",
            "video 1/1 (frame 427/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2721.8ms\n",
            "video 1/1 (frame 428/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3921.8ms\n",
            "video 1/1 (frame 429/592) /content/people-counting.mp4: 384x640 1 person, 2 potted plants, 2795.8ms\n",
            "video 1/1 (frame 430/592) /content/people-counting.mp4: 384x640 1 person, 2 potted plants, 2777.8ms\n",
            "video 1/1 (frame 431/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2710.8ms\n",
            "video 1/1 (frame 432/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3953.3ms\n",
            "video 1/1 (frame 433/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2764.4ms\n",
            "video 1/1 (frame 434/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2772.8ms\n",
            "video 1/1 (frame 435/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2762.9ms\n",
            "video 1/1 (frame 436/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3690.6ms\n",
            "video 1/1 (frame 437/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2854.8ms\n",
            "video 1/1 (frame 438/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2805.5ms\n",
            "video 1/1 (frame 439/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2773.1ms\n",
            "video 1/1 (frame 440/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3456.7ms\n",
            "video 1/1 (frame 441/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3188.1ms\n",
            "video 1/1 (frame 442/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2828.7ms\n",
            "video 1/1 (frame 443/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2751.4ms\n",
            "video 1/1 (frame 444/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3199.5ms\n",
            "video 1/1 (frame 445/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3432.2ms\n",
            "video 1/1 (frame 446/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2774.5ms\n",
            "video 1/1 (frame 447/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2768.4ms\n",
            "video 1/1 (frame 448/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3211.2ms\n",
            "video 1/1 (frame 449/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3665.4ms\n",
            "video 1/1 (frame 450/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2834.8ms\n",
            "video 1/1 (frame 451/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2809.1ms\n",
            "video 1/1 (frame 452/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2997.1ms\n",
            "video 1/1 (frame 453/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3761.8ms\n",
            "video 1/1 (frame 454/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2763.6ms\n",
            "video 1/1 (frame 455/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2783.2ms\n",
            "video 1/1 (frame 456/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2782.7ms\n",
            "video 1/1 (frame 457/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3994.4ms\n",
            "video 1/1 (frame 458/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2746.2ms\n",
            "video 1/1 (frame 459/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2766.5ms\n",
            "video 1/1 (frame 460/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2751.9ms\n",
            "video 1/1 (frame 461/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3991.2ms\n",
            "video 1/1 (frame 462/592) /content/people-counting.mp4: 384x640 1 potted plant, 2732.1ms\n",
            "video 1/1 (frame 463/592) /content/people-counting.mp4: 384x640 1 potted plant, 2697.2ms\n",
            "video 1/1 (frame 464/592) /content/people-counting.mp4: 384x640 1 potted plant, 2744.4ms\n",
            "video 1/1 (frame 465/592) /content/people-counting.mp4: 384x640 1 potted plant, 3752.1ms\n",
            "video 1/1 (frame 466/592) /content/people-counting.mp4: 384x640 1 potted plant, 2859.8ms\n",
            "video 1/1 (frame 467/592) /content/people-counting.mp4: 384x640 1 potted plant, 2737.1ms\n",
            "video 1/1 (frame 468/592) /content/people-counting.mp4: 384x640 1 potted plant, 2726.0ms\n",
            "video 1/1 (frame 469/592) /content/people-counting.mp4: 384x640 1 potted plant, 3405.1ms\n",
            "video 1/1 (frame 470/592) /content/people-counting.mp4: 384x640 1 potted plant, 3172.6ms\n",
            "video 1/1 (frame 471/592) /content/people-counting.mp4: 384x640 1 potted plant, 2772.8ms\n",
            "video 1/1 (frame 472/592) /content/people-counting.mp4: 384x640 1 potted plant, 2766.6ms\n",
            "video 1/1 (frame 473/592) /content/people-counting.mp4: 384x640 1 potted plant, 3170.9ms\n",
            "video 1/1 (frame 474/592) /content/people-counting.mp4: 384x640 1 potted plant, 3389.5ms\n",
            "video 1/1 (frame 475/592) /content/people-counting.mp4: 384x640 1 potted plant, 2740.9ms\n",
            "video 1/1 (frame 476/592) /content/people-counting.mp4: 384x640 1 potted plant, 2749.3ms\n",
            "video 1/1 (frame 477/592) /content/people-counting.mp4: 384x640 1 potted plant, 2832.9ms\n",
            "video 1/1 (frame 478/592) /content/people-counting.mp4: 384x640 1 potted plant, 3787.7ms\n",
            "video 1/1 (frame 479/592) /content/people-counting.mp4: 384x640 1 potted plant, 2778.4ms\n",
            "video 1/1 (frame 480/592) /content/people-counting.mp4: 384x640 1 potted plant, 2764.3ms\n",
            "video 1/1 (frame 481/592) /content/people-counting.mp4: 384x640 1 potted plant, 2797.0ms\n",
            "video 1/1 (frame 482/592) /content/people-counting.mp4: 384x640 1 potted plant, 3899.4ms\n",
            "video 1/1 (frame 483/592) /content/people-counting.mp4: 384x640 1 potted plant, 2767.3ms\n",
            "video 1/1 (frame 484/592) /content/people-counting.mp4: 384x640 1 potted plant, 2744.8ms\n",
            "video 1/1 (frame 485/592) /content/people-counting.mp4: 384x640 1 potted plant, 2757.2ms\n",
            "video 1/1 (frame 486/592) /content/people-counting.mp4: 384x640 1 potted plant, 3921.3ms\n",
            "video 1/1 (frame 487/592) /content/people-counting.mp4: 384x640 1 potted plant, 2747.4ms\n",
            "video 1/1 (frame 488/592) /content/people-counting.mp4: 384x640 1 potted plant, 2736.3ms\n",
            "video 1/1 (frame 489/592) /content/people-counting.mp4: 384x640 1 potted plant, 2755.6ms\n",
            "video 1/1 (frame 490/592) /content/people-counting.mp4: 384x640 1 potted plant, 3809.6ms\n",
            "video 1/1 (frame 491/592) /content/people-counting.mp4: 384x640 1 potted plant, 2849.7ms\n",
            "video 1/1 (frame 492/592) /content/people-counting.mp4: 384x640 1 potted plant, 2734.6ms\n",
            "video 1/1 (frame 493/592) /content/people-counting.mp4: 384x640 1 potted plant, 2730.1ms\n",
            "video 1/1 (frame 494/592) /content/people-counting.mp4: 384x640 1 potted plant, 3371.0ms\n",
            "video 1/1 (frame 495/592) /content/people-counting.mp4: 384x640 1 potted plant, 3267.7ms\n",
            "video 1/1 (frame 496/592) /content/people-counting.mp4: 384x640 1 potted plant, 2787.2ms\n",
            "video 1/1 (frame 497/592) /content/people-counting.mp4: 384x640 1 potted plant, 2766.4ms\n",
            "video 1/1 (frame 498/592) /content/people-counting.mp4: 384x640 1 potted plant, 3228.6ms\n",
            "video 1/1 (frame 499/592) /content/people-counting.mp4: 384x640 1 potted plant, 3400.8ms\n",
            "video 1/1 (frame 500/592) /content/people-counting.mp4: 384x640 1 potted plant, 2789.0ms\n",
            "video 1/1 (frame 501/592) /content/people-counting.mp4: 384x640 1 potted plant, 2738.1ms\n",
            "video 1/1 (frame 502/592) /content/people-counting.mp4: 384x640 1 potted plant, 2922.6ms\n",
            "video 1/1 (frame 503/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3645.2ms\n",
            "video 1/1 (frame 504/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2712.9ms\n",
            "video 1/1 (frame 505/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2730.9ms\n",
            "video 1/1 (frame 506/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2717.6ms\n",
            "video 1/1 (frame 507/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3889.4ms\n",
            "video 1/1 (frame 508/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2753.6ms\n",
            "video 1/1 (frame 509/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2742.1ms\n",
            "video 1/1 (frame 510/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2760.8ms\n",
            "video 1/1 (frame 511/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3880.9ms\n",
            "video 1/1 (frame 512/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2757.6ms\n",
            "video 1/1 (frame 513/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2740.6ms\n",
            "video 1/1 (frame 514/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2771.9ms\n",
            "video 1/1 (frame 515/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3898.9ms\n",
            "video 1/1 (frame 516/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2772.8ms\n",
            "video 1/1 (frame 517/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2791.5ms\n",
            "video 1/1 (frame 518/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2808.8ms\n",
            "video 1/1 (frame 519/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3672.3ms\n",
            "video 1/1 (frame 520/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2967.8ms\n",
            "video 1/1 (frame 521/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2767.6ms\n",
            "video 1/1 (frame 522/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2758.5ms\n",
            "video 1/1 (frame 523/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 5085.3ms\n",
            "video 1/1 (frame 524/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 5064.7ms\n",
            "video 1/1 (frame 525/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3301.9ms\n",
            "video 1/1 (frame 526/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3817.8ms\n",
            "video 1/1 (frame 527/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 1 clock, 2910.3ms\n",
            "video 1/1 (frame 528/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 1 clock, 2776.7ms\n",
            "video 1/1 (frame 529/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 1 clock, 2779.0ms\n",
            "video 1/1 (frame 530/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3502.0ms\n",
            "video 1/1 (frame 531/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3124.0ms\n",
            "video 1/1 (frame 532/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2954.9ms\n",
            "video 1/1 (frame 533/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2793.5ms\n",
            "video 1/1 (frame 534/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3461.3ms\n",
            "video 1/1 (frame 535/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3269.0ms\n",
            "video 1/1 (frame 536/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2788.7ms\n",
            "video 1/1 (frame 537/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2762.2ms\n",
            "video 1/1 (frame 538/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3176.9ms\n",
            "video 1/1 (frame 539/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3508.4ms\n",
            "video 1/1 (frame 540/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2785.0ms\n",
            "video 1/1 (frame 541/592) /content/people-counting.mp4: 384x640 1 person, 2773.5ms\n",
            "video 1/1 (frame 542/592) /content/people-counting.mp4: 384x640 1 person, 3047.3ms\n",
            "video 1/1 (frame 543/592) /content/people-counting.mp4: 384x640 1 person, 3651.2ms\n",
            "video 1/1 (frame 544/592) /content/people-counting.mp4: 384x640 1 person, 2798.2ms\n",
            "video 1/1 (frame 545/592) /content/people-counting.mp4: 384x640 1 person, 2824.1ms\n",
            "video 1/1 (frame 546/592) /content/people-counting.mp4: 384x640 1 person, 2867.4ms\n",
            "video 1/1 (frame 547/592) /content/people-counting.mp4: 384x640 1 person, 3941.2ms\n",
            "video 1/1 (frame 548/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2771.3ms\n",
            "video 1/1 (frame 549/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2796.0ms\n",
            "video 1/1 (frame 550/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2804.5ms\n",
            "video 1/1 (frame 551/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 4082.3ms\n",
            "video 1/1 (frame 552/592) /content/people-counting.mp4: 384x640 1 person, 2750.3ms\n",
            "video 1/1 (frame 553/592) /content/people-counting.mp4: 384x640 1 person, 2842.2ms\n",
            "video 1/1 (frame 554/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2843.4ms\n",
            "video 1/1 (frame 555/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 4116.4ms\n",
            "video 1/1 (frame 556/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2810.9ms\n",
            "video 1/1 (frame 557/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2758.9ms\n",
            "video 1/1 (frame 558/592) /content/people-counting.mp4: 384x640 1 person, 2 potted plants, 2779.8ms\n",
            "video 1/1 (frame 559/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3991.2ms\n",
            "video 1/1 (frame 560/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2817.5ms\n",
            "video 1/1 (frame 561/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 2888.8ms\n",
            "video 1/1 (frame 562/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 2847.4ms\n",
            "video 1/1 (frame 563/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 3862.7ms\n",
            "video 1/1 (frame 564/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 2851.9ms\n",
            "video 1/1 (frame 565/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 2746.7ms\n",
            "video 1/1 (frame 566/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 2810.4ms\n",
            "video 1/1 (frame 567/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 3569.5ms\n",
            "video 1/1 (frame 568/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 3069.0ms\n",
            "video 1/1 (frame 569/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 2828.2ms\n",
            "video 1/1 (frame 570/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 2942.1ms\n",
            "video 1/1 (frame 571/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 3513.3ms\n",
            "video 1/1 (frame 572/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 3192.5ms\n",
            "video 1/1 (frame 573/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 2874.9ms\n",
            "video 1/1 (frame 574/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 2852.3ms\n",
            "video 1/1 (frame 575/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 3345.8ms\n",
            "video 1/1 (frame 576/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 3242.8ms\n",
            "video 1/1 (frame 577/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 2779.7ms\n",
            "video 1/1 (frame 578/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 2749.2ms\n",
            "video 1/1 (frame 579/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 3047.8ms\n",
            "video 1/1 (frame 580/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 3603.0ms\n",
            "video 1/1 (frame 581/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 2749.7ms\n",
            "video 1/1 (frame 582/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 2750.3ms\n",
            "video 1/1 (frame 583/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 2755.1ms\n",
            "video 1/1 (frame 584/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 3831.2ms\n",
            "video 1/1 (frame 585/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 2784.0ms\n",
            "video 1/1 (frame 586/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2749.8ms\n",
            "video 1/1 (frame 587/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 2736.3ms\n",
            "video 1/1 (frame 588/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3885.7ms\n",
            "video 1/1 (frame 589/592) /content/people-counting.mp4: 384x640 2 persons, 1 potted plant, 2748.0ms\n",
            "video 1/1 (frame 590/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 2742.5ms\n",
            "video 1/1 (frame 591/592) /content/people-counting.mp4: 384x640 1 potted plant, 2740.5ms\n",
            "video 1/1 (frame 592/592) /content/people-counting.mp4: 384x640 1 person, 1 potted plant, 3867.4ms\n",
            "Speed: 5.4ms preprocess, 3048.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing complete. Output video saved.\n",
            "Total: 5, Entering: 4, Exiting: 1\n"
          ]
        }
      ],
      "source": [
        "import supervision as sv\n",
        "from ultralytics import YOLO\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# Video Paths\n",
        "TARGET_VIDEO_PATH = 'output_video.mp4'\n",
        "\n",
        "# Define two polygonal areas\n",
        "area1 = np.array([(1169, 1678+50), (1942, 2025+50), (1816, 2102+50), (1085, 1703+50)], np.int32)\n",
        "area2 = np.array([(1040, 1710+50), (1771, 2117+50), (1673, 2142+50), (981, 1713+50)], np.int32)\n",
        "\n",
        "# Initialize counts\n",
        "total_count = 0\n",
        "entering_count = 0\n",
        "exiting_count = 0\n",
        "tracker_states = {}  # Track movement of each ID\n",
        "detection_data = {}  # Store detection data for JSON\n",
        "\n",
        "# Open video info\n",
        "video_info = sv.VideoInfo.from_video_path(SOURCE_VIDEO_PATH)\n",
        "\n",
        "# Video Sink\n",
        "with sv.VideoSink(TARGET_VIDEO_PATH, video_info) as sink:\n",
        "    for frame_number, result in enumerate(\n",
        "        model.track(source=SOURCE_VIDEO_PATH, tracker='bytetrack.yaml', show=False, stream=True, persist=True)\n",
        "    ):\n",
        "        frame = result.orig_img\n",
        "        detections = sv.Detections.from_yolov8(result)\n",
        "\n",
        "        # Only keep people (COCO class 0)\n",
        "        PEOPLE_CLASS_ID = 0\n",
        "        people_detections = []\n",
        "\n",
        "        if result.boxes.id is not None:  # Ensure we have tracker IDs\n",
        "            tracker_ids = result.boxes.id.cpu().numpy().astype(int)  # Get tracker IDs\n",
        "            for i, (bbox, confidence, class_id) in enumerate(zip(detections.xyxy, detections.confidence, detections.class_id)):\n",
        "                if class_id == PEOPLE_CLASS_ID:\n",
        "                    tracker_id = tracker_ids[i] if i < len(tracker_ids) else None\n",
        "                    people_detections.append((bbox, confidence, class_id, tracker_id))\n",
        "\n",
        "        # Process detections\n",
        "        for bbox, confidence, class_id, tracker_id in people_detections:\n",
        "            x1, y1, x2, y2 = bbox  # Get bounding box coordinates\n",
        "            bottom_right = (int(x2), int(y2))  # Bottom-right corner\n",
        "\n",
        "            # Check if the bottom-right corner is in an area\n",
        "            in_area1 = cv2.pointPolygonTest(area1, bottom_right, False) >= 0\n",
        "            in_area2 = cv2.pointPolygonTest(area2, bottom_right, False) >= 0\n",
        "\n",
        "            if tracker_id not in tracker_states:\n",
        "                tracker_states[tracker_id] = []  # Initialize state tracking\n",
        "                detection_data[tracker_id] = {  # Initialize detection data\n",
        "                    \"tracker_id\": int(tracker_id),  # Convert to standard Python int\n",
        "                    \"entry_time\": None,\n",
        "                    \"exit_time\": None,\n",
        "                    \"bbox_history\": [],\n",
        "                    \"confidence\": float(confidence),  # Convert to standard Python float\n",
        "                    \"class_id\": int(class_id)  # Convert to standard Python int\n",
        "                }\n",
        "\n",
        "            # Track order of area entry\n",
        "            if in_area2 and \"area2\" not in tracker_states[tracker_id]:\n",
        "                tracker_states[tracker_id].append(\"area2\")\n",
        "                if tracker_states[tracker_id] == [\"area2\"]:\n",
        "                    detection_data[tracker_id][\"entry_time\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "            if in_area1 and \"area1\" not in tracker_states[tracker_id]:\n",
        "                tracker_states[tracker_id].append(\"area1\")\n",
        "                if tracker_states[tracker_id] == [\"area1\"]:\n",
        "                    detection_data[tracker_id][\"exit_time\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "            # Determine if entering or exiting\n",
        "            if tracker_states[tracker_id] == [\"area2\", \"area1\"]:\n",
        "                entering_count += 1\n",
        "                tracker_states[tracker_id] = []  # Reset state after count\n",
        "            elif tracker_states[tracker_id] == [\"area1\", \"area2\"]:\n",
        "                exiting_count += 1\n",
        "                tracker_states[tracker_id] = []  # Reset state after count\n",
        "\n",
        "            total_count = entering_count + exiting_count\n",
        "\n",
        "            # Update bbox history\n",
        "            detection_data[tracker_id][\"bbox_history\"].append({\n",
        "                \"frame_number\": int(frame_number),  # Convert to standard Python int\n",
        "                \"bbox\": [float(x1), float(y1), float(x2), float(y2)],  # Convert to standard Python floats\n",
        "                \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "            })\n",
        "\n",
        "            # Draw bounding box and ID\n",
        "            cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)\n",
        "            cv2.putText(frame, f\"ID: {tracker_id}\", (int(x1), int(y1) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
        "\n",
        "            # Draw bottom-right corner marker\n",
        "            cv2.circle(frame, bottom_right, 5, (0, 0, 255), -1)\n",
        "\n",
        "        # Draw polygons on the frame\n",
        "        cv2.polylines(frame, [area1], isClosed=True, color=(255, 0, 0), thickness=2)\n",
        "        cv2.polylines(frame, [area2], isClosed=True, color=(0, 255, 0), thickness=2)\n",
        "\n",
        "        # Display counts\n",
        "        cv2.putText(frame, f\"Total: {total_count}\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
        "        cv2.putText(frame, f\"Entering: {entering_count}\", (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "        cv2.putText(frame, f\"Exiting: {exiting_count}\", (50, 150), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
        "\n",
        "        # Save frame to video\n",
        "        sink.write_frame(frame)\n",
        "\n",
        "# Prepare JSON data\n",
        "json_data = {\n",
        "    \"summary\": {\n",
        "        \"total_people\": int(total_count),  # Convert to standard Python int\n",
        "        \"total_entering\": int(entering_count),  # Convert to standard Python int\n",
        "        \"total_exiting\": int(exiting_count),  # Convert to standard Python int\n",
        "        \"total_timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    },\n",
        "    \"detections\": {\n",
        "        # Convert NumPy int64 keys to standard Python int\n",
        "        int(tracker_id): data\n",
        "        for tracker_id, data in detection_data.items()\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save all data to a single JSON file\n",
        "json_path = \"detection_data.json\"\n",
        "with open(json_path, \"w\") as json_file:\n",
        "    json.dump(json_data, json_file, indent=4)\n",
        "\n",
        "print(\"Processing complete. Output video saved.\")\n",
        "print(f\"Total: {total_count}, Entering: {entering_count}, Exiting: {exiting_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Enter Exit People Count and Age Gender Detectin\n",
        "\n",
        "##### Enter Exit Count + Age Gender\n",
        "\n",
        "* Add age gender detection to above code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install deepface\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import supervision as sv\n",
        "from ultralytics import YOLO\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import json\n",
        "from datetime import datetime\n",
        "from deepface import DeepFace\n",
        "\n",
        "# Video Paths\n",
        "TARGET_VIDEO_PATH = 'output_video.mp4'\n",
        "\n",
        "# Define two polygonal areas\n",
        "area1 = np.array([(1169, 1678+50), (1942, 2025+50), (1816, 2102+50), (1085, 1703+50)], np.int32)\n",
        "area2 = np.array([(1040, 1710+50), (1771, 2117+50), (1673, 2142+50), (981, 1713+50)], np.int32)\n",
        "\n",
        "# Initialize counts\n",
        "total_count = 0\n",
        "entering_count = 0\n",
        "exiting_count = 0\n",
        "tracker_states = {}  # Track movement of each ID\n",
        "detection_data = {}  # Store detection data for JSON\n",
        "\n",
        "# Open video info\n",
        "video_info = sv.VideoInfo.from_video_path(SOURCE_VIDEO_PATH)\n",
        "\n",
        "# Function to get age and gender using DeepFace\n",
        "def predict_age_gender(face):\n",
        "    try:\n",
        "        analysis = DeepFace.analyze(face, actions=[\"age\", \"gender\"], enforce_detection=False)\n",
        "        age = analysis[0][\"age\"]\n",
        "        gender = analysis[0][\"dominant_gender\"]\n",
        "        return gender, age\n",
        "    except Exception as e:\n",
        "        print(f\"Error in age/gender prediction: {e}\")\n",
        "        return \"Unknown\", \"Unknown\"\n",
        "\n",
        "# Video Sink\n",
        "with sv.VideoSink(TARGET_VIDEO_PATH, video_info) as sink:\n",
        "    for frame_number, result in enumerate(\n",
        "        model.track(source=SOURCE_VIDEO_PATH, tracker='bytetrack.yaml', show=False, stream=True, persist=True)\n",
        "    ):\n",
        "        frame = result.orig_img\n",
        "        detections = sv.Detections.from_yolov8(result)\n",
        "\n",
        "        # Only keep people (COCO class 0)\n",
        "        PEOPLE_CLASS_ID = 0\n",
        "        people_detections = []\n",
        "\n",
        "        if result.boxes.id is not None:  # Ensure we have tracker IDs\n",
        "            tracker_ids = result.boxes.id.cpu().numpy().astype(int)  # Get tracker IDs\n",
        "            for i, (bbox, confidence, class_id) in enumerate(zip(detections.xyxy, detections.confidence, detections.class_id)):\n",
        "                if class_id == PEOPLE_CLASS_ID:\n",
        "                    tracker_id = tracker_ids[i] if i < len(tracker_ids) else None\n",
        "                    people_detections.append((bbox, confidence, class_id, tracker_id))\n",
        "\n",
        "        # Process detections\n",
        "        for bbox, confidence, class_id, tracker_id in people_detections:\n",
        "            x1, y1, x2, y2 = bbox  # Get bounding box coordinates\n",
        "            bottom_right = (int(x2), int(y2))  # Bottom-right corner\n",
        "\n",
        "            # Crop face for gender and age prediction\n",
        "            face = frame[int(y1):int(y2), int(x1):int(x2)]\n",
        "            gender, age = \"Unknown\", \"Unknown\"\n",
        "            if face.size > 0:\n",
        "                gender, age = predict_age_gender(face)\n",
        "\n",
        "            # Check if the bottom-right corner is in an area\n",
        "            in_area1 = cv2.pointPolygonTest(area1, bottom_right, False) >= 0\n",
        "            in_area2 = cv2.pointPolygonTest(area2, bottom_right, False) >= 0\n",
        "\n",
        "            if tracker_id not in tracker_states:\n",
        "                tracker_states[tracker_id] = []  # Initialize state tracking\n",
        "                detection_data[tracker_id] = {  # Initialize detection data\n",
        "                    \"tracker_id\": int(tracker_id),  # Convert to standard Python int\n",
        "                    \"gender\": gender,\n",
        "                    \"age\": age,\n",
        "                    \"entry_time\": None,\n",
        "                    \"exit_time\": None,\n",
        "                    \"bbox_history\": [],\n",
        "                    \"confidence\": float(confidence),  # Convert to standard Python float\n",
        "                    \"class_id\": int(class_id)  # Convert to standard Python int\n",
        "                }\n",
        "\n",
        "            # Track order of area entry\n",
        "            if in_area2 and \"area2\" not in tracker_states[tracker_id]:\n",
        "                tracker_states[tracker_id].append(\"area2\")\n",
        "                if tracker_states[tracker_id] == [\"area2\"]:\n",
        "                    detection_data[tracker_id][\"entry_time\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "            if in_area1 and \"area1\" not in tracker_states[tracker_id]:\n",
        "                tracker_states[tracker_id].append(\"area1\")\n",
        "                if tracker_states[tracker_id] == [\"area1\"]:\n",
        "                    detection_data[tracker_id][\"exit_time\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "            # Determine if entering or exiting\n",
        "            if tracker_states[tracker_id] == [\"area2\", \"area1\"]:\n",
        "                entering_count += 1\n",
        "                tracker_states[tracker_id] = []  # Reset state after count\n",
        "            elif tracker_states[tracker_id] == [\"area1\", \"area2\"]:\n",
        "                exiting_count += 1\n",
        "                tracker_states[tracker_id] = []  # Reset state after count\n",
        "\n",
        "            total_count = entering_count + exiting_count\n",
        "\n",
        "            # Update bbox history\n",
        "            detection_data[tracker_id][\"bbox_history\"].append({\n",
        "                \"frame_number\": int(frame_number),  # Convert to standard Python int\n",
        "                \"bbox\": [float(x1), float(y1), float(x2), float(y2)],  # Convert to standard Python floats\n",
        "                \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "            })\n",
        "\n",
        "            # Draw bounding box and ID\n",
        "            cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)\n",
        "            label = f\"ID: {tracker_id} | {gender}, {age}\"\n",
        "            cv2.putText(frame, label, (int(x1), int(y1) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
        "\n",
        "            # Draw bottom-right corner marker\n",
        "            cv2.circle(frame, bottom_right, 5, (0, 0, 255), -1)\n",
        "\n",
        "        # Draw polygons on the frame\n",
        "        cv2.polylines(frame, [area1], isClosed=True, color=(255, 0, 0), thickness=2)\n",
        "        cv2.polylines(frame, [area2], isClosed=True, color=(0, 255, 0), thickness=2)\n",
        "\n",
        "        # Display counts\n",
        "        cv2.putText(frame, f\"Total: {total_count}\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
        "        cv2.putText(frame, f\"Entering: {entering_count}\", (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "        cv2.putText(frame, f\"Exiting: {exiting_count}\", (50, 150), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
        "\n",
        "        # Save frame to video\n",
        "        sink.write_frame(frame)\n",
        "\n",
        "# Prepare JSON data\n",
        "json_data = {\n",
        "    \"summary\": {\n",
        "        \"total_people\": int(total_count),  # Convert to standard Python int\n",
        "        \"total_entering\": int(entering_count),  # Convert to standard Python int\n",
        "        \"total_exiting\": int(exiting_count),  # Convert to standard Python int\n",
        "        \"total_timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    },\n",
        "    \"detections\": {\n",
        "        # Convert NumPy int64 keys to standard Python int\n",
        "        int(tracker_id): data\n",
        "        for tracker_id, data in detection_data.items()\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save all data to a single JSON file\n",
        "json_path = \"detection_data.json\"\n",
        "with open(json_path, \"w\") as json_file:\n",
        "    json.dump(json_data, json_file, indent=4)\n",
        "\n",
        "print(\"Processing complete. Output video saved.\")\n",
        "print(f\"Total: {total_count}, Entering: {entering_count}, Exiting: {exiting_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Get metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install ffmpeg-python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### Get video creation date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import ffmpeg\n",
        "\n",
        "def get_video_creation_time(video_path):\n",
        "    try:\n",
        "        probe = ffmpeg.probe(video_path)\n",
        "        \n",
        "        # Check both 'format' and 'streams' for creation_time\n",
        "        creation_time = None\n",
        "        \n",
        "        # 1. Check format tags\n",
        "        if 'format' in probe and 'tags' in probe['format']:\n",
        "            creation_time = probe['format']['tags'].get('creation_time')\n",
        "        \n",
        "        # 2. Fallback to stream tags\n",
        "        if not creation_time:\n",
        "            for stream in probe.get('streams', []):\n",
        "                if 'tags' in stream and 'creation_time' in stream['tags']:\n",
        "                    creation_time = stream['tags']['creation_time']\n",
        "                    break\n",
        "        \n",
        "        return creation_time\n",
        "    \n",
        "    except ffmpeg.Error as e:\n",
        "        print(f\"FFmpeg error: {e.stderr.decode('utf-8')}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Usage\n",
        "creation_time = get_video_creation_time(\"/content/people-counting.mp4\")\n",
        "if creation_time:\n",
        "    print(f\"Creation time: {creation_time}\")\n",
        "else:\n",
        "    print(\"No creation time found in metadata\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### Get all metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import ffmpeg\n",
        "from datetime import datetime, timezone\n",
        "import json\n",
        "\n",
        "def extract_video_metadata(video_path):\n",
        "    \"\"\"Extract all available metadata from a video file using ffmpeg-python.\"\"\"\n",
        "    try:\n",
        "        probe = ffmpeg.probe(video_path)\n",
        "        metadata = {}\n",
        "\n",
        "        # ===== 1. General Video Information =====\n",
        "        if \"format\" in probe:\n",
        "            format_info = probe[\"format\"]\n",
        "            metadata.update({\n",
        "                \"filename\": format_info.get(\"filename\"),\n",
        "                \"format_name\": format_info.get(\"format_name\"),\n",
        "                \"format_long_name\": format_info.get(\"format_long_name\"),\n",
        "                \"duration_seconds\": float(format_info.get(\"duration\", 0)),\n",
        "                \"size_bytes\": int(format_info.get(\"size\", 0)),\n",
        "                \"bitrate\": int(format_info.get(\"bit_rate\", 0)),\n",
        "            })\n",
        "\n",
        "            # Extract creation_time (if available)\n",
        "            if \"tags\" in format_info:\n",
        "                metadata.update({\n",
        "                    \"creation_time\": format_info[\"tags\"].get(\"creation_time\"),\n",
        "                    \"encoder\": format_info[\"tags\"].get(\"encoder\"),\n",
        "                })\n",
        "\n",
        "        # ===== 2. Video Stream Metadata =====\n",
        "        video_streams = [s for s in probe[\"streams\"] if s[\"codec_type\"] == \"video\"]\n",
        "        if video_streams:\n",
        "            video_info = video_streams[0]\n",
        "            metadata.update({\n",
        "                \"video_codec\": video_info.get(\"codec_name\"),\n",
        "                \"width\": int(video_info.get(\"width\", 0)),\n",
        "                \"height\": int(video_info.get(\"height\", 0)),\n",
        "                \"fps\": eval(video_info.get(\"avg_frame_rate\", \"0/1\")),  # e.g., \"30/1\" â†’ 30.0\n",
        "            })\n",
        "\n",
        "            # Extract device-specific metadata (iPhone, Android, etc.)\n",
        "            if \"tags\" in video_info:\n",
        "                metadata.update({\n",
        "                    \"device_model\": video_info[\"tags\"].get(\"com.apple.quicktime.model\"),\n",
        "                    \"software\": video_info[\"tags\"].get(\"software\"),\n",
        "                })\n",
        "\n",
        "        # ===== 3. Audio Stream Metadata =====\n",
        "        audio_streams = [s for s in probe[\"streams\"] if s[\"codec_type\"] == \"audio\"]\n",
        "        if audio_streams:\n",
        "            audio_info = audio_streams[0]\n",
        "            metadata.update({\n",
        "                \"audio_codec\": audio_info.get(\"codec_name\"),\n",
        "                \"sample_rate\": int(audio_info.get(\"sample_rate\", 0)),\n",
        "                \"channels\": int(audio_info.get(\"channels\", 0)),\n",
        "            })\n",
        "\n",
        "        # ===== 4. GPS Coordinates (if recorded) =====\n",
        "        if \"format\" in probe and \"tags\" in probe[\"format\"]:\n",
        "            tags = probe[\"format\"][\"tags\"]\n",
        "            if \"location\" in tags:  # Some Android devices store GPS here\n",
        "                metadata[\"gps_coordinates\"] = tags[\"location\"]\n",
        "            elif \"com.apple.quicktime.location.ISO6709\" in tags:  # iPhone GPS\n",
        "                metadata[\"gps_coordinates\"] = tags[\"com.apple.quicktime.location.ISO6709\"]\n",
        "\n",
        "        # ===== 5. Convert ISO Timestamp to Readable Format =====\n",
        "        if \"creation_time\" in metadata:\n",
        "            try:\n",
        "                dt = datetime.strptime(metadata[\"creation_time\"].split(\".\")[0], \"%Y-%m-%dT%H:%M:%S\")\n",
        "                dt = dt.replace(tzinfo=timezone.utc)\n",
        "                metadata[\"creation_time_utc\"] = dt.strftime(\"%Y-%m-%d %H:%M:%S UTC\")\n",
        "                metadata[\"creation_time_local\"] = dt.astimezone().strftime(\"%Y-%m-%d %H:%M:%S %Z\")\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        return metadata\n",
        "\n",
        "    except ffmpeg.Error as e:\n",
        "        print(f\"FFmpeg error: {e.stderr.decode('utf-8')}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Example Usage\n",
        "metadata = extract_video_metadata(\"/content/people-counting.mp4\")\n",
        "print(json.dumps(metadata, indent=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Video Metadata Extraction and  Carrying Object Identification\n",
        "\n",
        "##### Enter Exit Count + Age Gender + Video Metadata + Carrying Object\n",
        "\n",
        "* Add video metadata and carrying objects detection to above code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import supervision as sv\n",
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "import numpy as np\n",
        "import json\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "from deepface import DeepFace\n",
        "import ffmpeg\n",
        "from datetime import timezone\n",
        "\n",
        "# ===== Configuration =====\n",
        "TARGET_VIDEO_PATH = \"output_video_updated.mp4\"\n",
        "JSON_OUTPUT_PATH = \"detection_data_updated.json\"\n",
        "\n",
        "# Define polygonal areas for counting\n",
        "area1 = np.array([(1169, 1678+50), (1942, 2025+50), (1816, 2102+50), (1085, 1703+50)], np.int32)\n",
        "area2 = np.array([(1040, 1710+50), (1771, 2117+50), (1673, 2142+50), (981, 1713+50)], np.int32)\n",
        "\n",
        "# Object classes (COCO dataset)\n",
        "BAG_CLASSES = [24, 26, 28]  # backpack, handbag, suitcase\n",
        "CAT_CLASS = 15\n",
        "DOG_CLASS = 16\n",
        "\n",
        "# ===== Initialize =====\n",
        "total_count = 0\n",
        "entering_count = 0\n",
        "exiting_count = 0\n",
        "tracker_states = {}\n",
        "detection_data = {}\n",
        "\n",
        "# ===== Helper Functions =====\n",
        "def extract_video_metadata(video_path):\n",
        "    \"\"\"Extract all available metadata from a video file using ffmpeg-python.\"\"\"\n",
        "    try:\n",
        "        probe = ffmpeg.probe(video_path)\n",
        "        metadata = {}\n",
        "\n",
        "        # ===== 1. General Video Information =====\n",
        "        if \"format\" in probe:\n",
        "            format_info = probe[\"format\"]\n",
        "            metadata.update({\n",
        "                \"filename\": format_info.get(\"filename\"),\n",
        "                \"format_name\": format_info.get(\"format_name\"),\n",
        "                \"format_long_name\": format_info.get(\"format_long_name\"),\n",
        "                \"duration_seconds\": float(format_info.get(\"duration\", 0)),\n",
        "                \"size_bytes\": int(format_info.get(\"size\", 0)),\n",
        "                \"bitrate\": int(format_info.get(\"bit_rate\", 0)),\n",
        "            })\n",
        "\n",
        "            # Extract creation_time (if available)\n",
        "            if \"tags\" in format_info:\n",
        "                metadata.update({\n",
        "                    \"creation_time\": format_info[\"tags\"].get(\"creation_time\"),\n",
        "                    \"encoder\": format_info[\"tags\"].get(\"encoder\"),\n",
        "                })\n",
        "\n",
        "        # ===== 2. Video Stream Metadata =====\n",
        "        video_streams = [s for s in probe[\"streams\"] if s[\"codec_type\"] == \"video\"]\n",
        "        if video_streams:\n",
        "            video_info = video_streams[0]\n",
        "            metadata.update({\n",
        "                \"video_codec\": video_info.get(\"codec_name\"),\n",
        "                \"width\": int(video_info.get(\"width\", 0)),\n",
        "                \"height\": int(video_info.get(\"height\", 0)),\n",
        "                \"fps\": eval(video_info.get(\"avg_frame_rate\", \"0/1\")),  # e.g., \"30/1\" â†’ 30.0\n",
        "            })\n",
        "\n",
        "            # Extract device-specific metadata (iPhone, Android, etc.)\n",
        "            if \"tags\" in video_info:\n",
        "                metadata.update({\n",
        "                    \"device_model\": video_info[\"tags\"].get(\"com.apple.quicktime.model\"),\n",
        "                    \"software\": video_info[\"tags\"].get(\"software\"),\n",
        "                })\n",
        "\n",
        "        # ===== 3. Audio Stream Metadata =====\n",
        "        audio_streams = [s for s in probe[\"streams\"] if s[\"codec_type\"] == \"audio\"]\n",
        "        if audio_streams:\n",
        "            audio_info = audio_streams[0]\n",
        "            metadata.update({\n",
        "                \"audio_codec\": audio_info.get(\"codec_name\"),\n",
        "                \"sample_rate\": int(audio_info.get(\"sample_rate\", 0)),\n",
        "                \"channels\": int(audio_info.get(\"channels\", 0)),\n",
        "            })\n",
        "\n",
        "        # ===== 4. GPS Coordinates (if recorded) =====\n",
        "        if \"format\" in probe and \"tags\" in probe[\"format\"]:\n",
        "            tags = probe[\"format\"][\"tags\"]\n",
        "            if \"location\" in tags:  # Some Android devices store GPS here\n",
        "                metadata[\"gps_coordinates\"] = tags[\"location\"]\n",
        "            elif \"com.apple.quicktime.location.ISO6709\" in tags:  # iPhone GPS\n",
        "                metadata[\"gps_coordinates\"] = tags[\"com.apple.quicktime.location.ISO6709\"]\n",
        "\n",
        "        # ===== 5. Convert ISO Timestamp to Readable Format =====\n",
        "        if \"creation_time\" in metadata:\n",
        "            try:\n",
        "                dt = datetime.strptime(metadata[\"creation_time\"].split(\".\")[0], \"%Y-%m-%dT%H:%M:%S\")\n",
        "                dt = dt.replace(tzinfo=timezone.utc)\n",
        "                metadata[\"creation_time_utc\"] = dt.strftime(\"%Y-%m-%d %H:%M:%S UTC\")\n",
        "                metadata[\"creation_time_local\"] = dt.astimezone().strftime(\"%Y-%m-%d %H:%M:%S %Z\")\n",
        "                metadata[\"recording_time\"] = metadata[\"creation_time_local\"]  # For backward compatibility\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        return metadata\n",
        "\n",
        "    except ffmpeg.Error as e:\n",
        "        print(f\"FFmpeg error: {e.stderr.decode('utf-8')}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def predict_age_gender(face_img):\n",
        "    \"\"\"Predict age and gender using DeepFace\"\"\"\n",
        "    try:\n",
        "        analysis = DeepFace.analyze(face_img, actions=[\"age\", \"gender\"], enforce_detection=False)\n",
        "        return analysis[0][\"dominant_gender\"], analysis[0][\"age\"]\n",
        "    except Exception:\n",
        "        return \"Unknown\", \"Unknown\"\n",
        "\n",
        "def is_carried(obj_bbox, person_bbox):\n",
        "    \"\"\"Check if an object is being carried by a person\"\"\"\n",
        "    px1, py1, px2, py2 = person_bbox\n",
        "    ox1, oy1, ox2, oy2 = obj_bbox\n",
        "    obj_center_y = (oy1 + oy2) / 2\n",
        "    lower_half_threshold = py1 + (py2 - py1) * 0.6\n",
        "    overlap = (ox1 > px1) and (ox2 < px2) and (oy1 > py1) and (oy2 < py2)\n",
        "    in_carry_position = obj_center_y > lower_half_threshold\n",
        "    return overlap and in_carry_position\n",
        "\n",
        "# ===== Main Processing =====\n",
        "video_metadata = extract_video_metadata(SOURCE_VIDEO_PATH)\n",
        "video_info = sv.VideoInfo.from_video_path(SOURCE_VIDEO_PATH)\n",
        "\n",
        "# Get recording time from metadata or use current time as fallback\n",
        "try:\n",
        "    recording_time = datetime.strptime(video_metadata[\"creation_time\"].split(\".\")[0], \"%Y-%m-%dT%H:%M:%S\")\n",
        "    recording_time = recording_time.replace(tzinfo=timezone.utc)\n",
        "except (KeyError, ValueError):\n",
        "    print(\"Warning: Using current time as recording time fallback\")\n",
        "    recording_time = datetime.now(timezone.utc)\n",
        "\n",
        "with sv.VideoSink(TARGET_VIDEO_PATH, video_info) as sink:\n",
        "    for frame_number, result in enumerate(\n",
        "        model.track(source=SOURCE_VIDEO_PATH, tracker=\"bytetrack.yaml\", show=False, stream=True, persist=True)\n",
        "    ):\n",
        "        frame = result.orig_img\n",
        "        detections = sv.Detections.from_yolov8(result)\n",
        "\n",
        "        # Separate detections\n",
        "        people = []\n",
        "        objects = []\n",
        "        if result.boxes.id is not None:\n",
        "            tracker_ids = result.boxes.id.cpu().numpy().astype(int)\n",
        "            for i, (bbox, conf, class_id) in enumerate(zip(detections.xyxy, detections.confidence, detections.class_id)):\n",
        "                if class_id == 0:  # Person\n",
        "                    tracker_id = tracker_ids[i] if i < len(tracker_ids) else None\n",
        "                    people.append((bbox, conf, tracker_id))\n",
        "                elif class_id in BAG_CLASSES + [CAT_CLASS, DOG_CLASS]:\n",
        "                    objects.append((bbox, conf, class_id))\n",
        "\n",
        "        # Process each person\n",
        "        for person_bbox, confidence, tracker_id in people:\n",
        "            x1, y1, x2, y2 = person_bbox\n",
        "            bottom_right = (int(x2), int(y2))\n",
        "\n",
        "            # Age/gender detection\n",
        "            face_roi = frame[int(y1):int(y2), int(x1):int(x2)]\n",
        "            gender, age = predict_age_gender(face_roi)\n",
        "\n",
        "            # Check for carried items\n",
        "            carried_items = []\n",
        "            for obj_bbox, _, class_id in objects:\n",
        "                if is_carried(obj_bbox, person_bbox):\n",
        "                    if class_id in BAG_CLASSES:\n",
        "                        carried_items.append(\"bag\")\n",
        "                    elif class_id == CAT_CLASS:\n",
        "                        carried_items.append(\"cat\")\n",
        "                    elif class_id == DOG_CLASS:\n",
        "                        carried_items.append(\"dog\")\n",
        "\n",
        "            # Initialize or update person data\n",
        "            if tracker_id not in tracker_states:\n",
        "                tracker_states[tracker_id] = []\n",
        "                detection_data[tracker_id] = {\n",
        "                    \"tracker_id\": int(tracker_id),\n",
        "                    \"gender\": gender,\n",
        "                    \"age\": age,\n",
        "                    \"carrying\": carried_items if carried_items else \"no objects\",\n",
        "                    \"entry_time\": None,\n",
        "                    \"exit_time\": None,\n",
        "                    \"entry_frame\": None,\n",
        "                    \"exit_frame\": None,\n",
        "                    \"bbox_history\": [],\n",
        "                    \"confidence\": float(confidence)\n",
        "                }\n",
        "\n",
        "            # Area crossing logic\n",
        "            in_area1 = cv2.pointPolygonTest(area1, bottom_right, False) >= 0\n",
        "            in_area2 = cv2.pointPolygonTest(area2, bottom_right, False) >= 0\n",
        "\n",
        "            if in_area2 and \"area2\" not in tracker_states[tracker_id]:\n",
        "                tracker_states[tracker_id].append(\"area2\")\n",
        "                if tracker_states[tracker_id] == [\"area2\"]:\n",
        "                    entry_time = recording_time + timedelta(seconds=frame_number / video_info.fps)\n",
        "                    detection_data[tracker_id][\"entry_time\"] = entry_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "                    detection_data[tracker_id][\"entry_frame\"] = frame_number\n",
        "\n",
        "            if in_area1 and \"area1\" not in tracker_states[tracker_id]:\n",
        "                tracker_states[tracker_id].append(\"area1\")\n",
        "                if tracker_states[tracker_id] == [\"area1\"]:\n",
        "                    exit_time = recording_time + timedelta(seconds=frame_number / video_info.fps)\n",
        "                    detection_data[tracker_id][\"exit_time\"] = exit_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "                    detection_data[tracker_id][\"exit_frame\"] = frame_number\n",
        "\n",
        "            # Update counts\n",
        "            if tracker_states[tracker_id] == [\"area2\", \"area1\"]:\n",
        "                entering_count += 1\n",
        "                tracker_states[tracker_id] = []\n",
        "            elif tracker_states[tracker_id] == [\"area1\", \"area2\"]:\n",
        "                exiting_count += 1\n",
        "                tracker_states[tracker_id] = []\n",
        "\n",
        "            total_count = entering_count + exiting_count\n",
        "\n",
        "            # Store bounding box history\n",
        "            detection_data[tracker_id][\"bbox_history\"].append({\n",
        "                \"frame_number\": int(frame_number),\n",
        "                \"bbox\": [float(x1), float(y1), float(x2), float(y2)],\n",
        "                \"timestamp\": (recording_time + timedelta(seconds=frame_number / video_info.fps)).strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                \"carrying\": carried_items if carried_items else \"no objects\"\n",
        "            })\n",
        "\n",
        "            # Draw bounding box and label\n",
        "            carrying_str = \", \".join(carried_items) if carried_items else \"no objects\"\n",
        "            label = f\"ID: {tracker_id} | {gender}, {age} | Carrying: {carrying_str}\"\n",
        "            cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)\n",
        "            cv2.putText(frame, label, (int(x1), int(y1) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
        "\n",
        "        # Draw counting areas and counters\n",
        "        cv2.polylines(frame, [area1], isClosed=True, color=(255, 0, 0), thickness=2)\n",
        "        cv2.polylines(frame, [area2], isClosed=True, color=(0, 255, 0), thickness=2)\n",
        "        cv2.putText(frame, f\"Total: {total_count}\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
        "        cv2.putText(frame, f\"Entering: {entering_count}\", (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "        cv2.putText(frame, f\"Exiting: {exiting_count}\", (50, 150), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
        "\n",
        "        # Write frame to output video\n",
        "        sink.write_frame(frame)\n",
        "\n",
        "# ===== Save Results =====\n",
        "json_output = {\n",
        "    \"video_metadata\": video_metadata,\n",
        "    \"processing_time\": datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S %Z\"),\n",
        "    \"summary\": {\n",
        "        \"total_people\": int(total_count),\n",
        "        \"total_entering\": int(entering_count),\n",
        "        \"total_exiting\": int(exiting_count),\n",
        "        \"fps\": float(video_info.fps),\n",
        "        \"duration_seconds\": float(video_info.total_frames / video_info.fps)\n",
        "    },\n",
        "    \"detections\": {\n",
        "        int(tracker_id): data for tracker_id, data in detection_data.items()\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(JSON_OUTPUT_PATH, \"w\") as f:\n",
        "    json.dump(json_output, f, indent=4)\n",
        "\n",
        "print(f\"Processing complete. Results saved to {TARGET_VIDEO_PATH} and {JSON_OUTPUT_PATH}\")\n",
        "print(f\"Total: {total_count}, Entering: {entering_count}, Exiting: {exiting_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Face Mask Detection and Speed Calculation \n",
        "\n",
        "##### Enter Exit Count + Age Gender + Video Metadata + Carrying Object + Face Mask Detection + Speed Calculation \n",
        "\n",
        "* Add face mask detection and speed calculation to above code\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!git clone https://github.com/chandrikadeb7/Face-Mask-Detection.git #get Face mask detection pre trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import img_to_array\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model_path = \"Face-Mask-Detection/mask_detector.model\"\n",
        "mask_model = load_model(model_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import supervision as sv\n",
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "import numpy as np\n",
        "import json\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "from deepface import DeepFace\n",
        "import ffmpeg\n",
        "from datetime import timezone\n",
        "import math\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "\n",
        "\n",
        "# ===== Configuration =====\n",
        "TARGET_VIDEO_PATH = \"output_video_mask_speed.mp4\"\n",
        "JSON_OUTPUT_PATH = \"detection_data_mask_speed.json\"\n",
        "\n",
        "# Define polygonal areas for counting\n",
        "area1 = np.array([(1169, 1678+50), (1942, 2025+50), (1816, 2102+50), (1085, 1703+50)], np.int32)\n",
        "area2 = np.array([(1040, 1710+50), (1771, 2117+50), (1673, 2142+50), (981, 1713+50)], np.int32)\n",
        "\n",
        "# Object classes (COCO dataset)\n",
        "BAG_CLASSES = [24, 26, 28]  # backpack, handbag, suitcase\n",
        "CAT_CLASS = 15\n",
        "DOG_CLASS = 16\n",
        "\n",
        "# Mask detection labels\n",
        "MASK_LABELS = [\"mask\", \"no_mask\", \"improper\"]\n",
        "\n",
        "# ===== Initialize =====\n",
        "total_count = 0\n",
        "entering_count = 0\n",
        "exiting_count = 0\n",
        "tracker_states = {}\n",
        "detection_data = {}\n",
        "previous_positions = {}  # To track movement between frames\n",
        "\n",
        "# ===== Helper Functions =====\n",
        "def extract_video_metadata(video_path):\n",
        "    \"\"\"Extract all available metadata from a video file using ffmpeg-python.\"\"\"\n",
        "    try:\n",
        "        probe = ffmpeg.probe(video_path)\n",
        "        metadata = {}\n",
        "\n",
        "        # ===== 1. General Video Information =====\n",
        "        if \"format\" in probe:\n",
        "            format_info = probe[\"format\"]\n",
        "            metadata.update({\n",
        "                \"filename\": format_info.get(\"filename\"),\n",
        "                \"format_name\": format_info.get(\"format_name\"),\n",
        "                \"format_long_name\": format_info.get(\"format_long_name\"),\n",
        "                \"duration_seconds\": float(format_info.get(\"duration\", 0)),\n",
        "                \"size_bytes\": int(format_info.get(\"size\", 0)),\n",
        "                \"bitrate\": int(format_info.get(\"bit_rate\", 0)),\n",
        "            })\n",
        "\n",
        "            # Extract creation_time (if available)\n",
        "            if \"tags\" in format_info:\n",
        "                metadata.update({\n",
        "                    \"creation_time\": format_info[\"tags\"].get(\"creation_time\"),\n",
        "                    \"encoder\": format_info[\"tags\"].get(\"encoder\"),\n",
        "                })\n",
        "\n",
        "        # ===== 2. Video Stream Metadata =====\n",
        "        video_streams = [s for s in probe[\"streams\"] if s[\"codec_type\"] == \"video\"]\n",
        "        if video_streams:\n",
        "            video_info = video_streams[0]\n",
        "            metadata.update({\n",
        "                \"video_codec\": video_info.get(\"codec_name\"),\n",
        "                \"width\": int(video_info.get(\"width\", 0)),\n",
        "                \"height\": int(video_info.get(\"height\", 0)),\n",
        "                \"fps\": eval(video_info.get(\"avg_frame_rate\", \"0/1\")),  # e.g., \"30/1\" â†’ 30.0\n",
        "            })\n",
        "\n",
        "            # Extract device-specific metadata (iPhone, Android, etc.)\n",
        "            if \"tags\" in video_info:\n",
        "                metadata.update({\n",
        "                    \"device_model\": video_info[\"tags\"].get(\"com.apple.quicktime.model\"),\n",
        "                    \"software\": video_info[\"tags\"].get(\"software\"),\n",
        "                })\n",
        "\n",
        "        # ===== 3. Audio Stream Metadata =====\n",
        "        audio_streams = [s for s in probe[\"streams\"] if s[\"codec_type\"] == \"audio\"]\n",
        "        if audio_streams:\n",
        "            audio_info = audio_streams[0]\n",
        "            metadata.update({\n",
        "                \"audio_codec\": audio_info.get(\"codec_name\"),\n",
        "                \"sample_rate\": int(audio_info.get(\"sample_rate\", 0)),\n",
        "                \"channels\": int(audio_info.get(\"channels\", 0)),\n",
        "            })\n",
        "\n",
        "        # ===== 4. GPS Coordinates (if recorded) =====\n",
        "        if \"format\" in probe and \"tags\" in probe[\"format\"]:\n",
        "            tags = probe[\"format\"][\"tags\"]\n",
        "            if \"location\" in tags:  # Some Android devices store GPS here\n",
        "                metadata[\"gps_coordinates\"] = tags[\"location\"]\n",
        "            elif \"com.apple.quicktime.location.ISO6709\" in tags:  # iPhone GPS\n",
        "                metadata[\"gps_coordinates\"] = tags[\"com.apple.quicktime.location.ISO6709\"]\n",
        "\n",
        "        # ===== 5. Convert ISO Timestamp to Readable Format =====\n",
        "        if \"creation_time\" in metadata:\n",
        "            try:\n",
        "                dt = datetime.strptime(metadata[\"creation_time\"].split(\".\")[0], \"%Y-%m-%dT%H:%M:%S\")\n",
        "                dt = dt.replace(tzinfo=timezone.utc)\n",
        "                metadata[\"creation_time_utc\"] = dt.strftime(\"%Y-%m-%d %H:%M:%S UTC\")\n",
        "                metadata[\"creation_time_local\"] = dt.astimezone().strftime(\"%Y-%m-%d %H:%M:%S %Z\")\n",
        "                metadata[\"recording_time\"] = metadata[\"creation_time_local\"]  # For backward compatibility\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        return metadata\n",
        "\n",
        "    except ffmpeg.Error as e:\n",
        "        print(f\"FFmpeg error: {e.stderr.decode('utf-8')}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def predict_age_gender(face_img):\n",
        "    \"\"\"Predict age and gender using DeepFace\"\"\"\n",
        "    try:\n",
        "        analysis = DeepFace.analyze(face_img, actions=[\"age\", \"gender\"], enforce_detection=False)\n",
        "        return analysis[0][\"dominant_gender\"], analysis[0][\"age\"]\n",
        "    except Exception:\n",
        "        return \"Unknown\", \"Unknown\"\n",
        "\n",
        "def detect_mask(face_img):\n",
        "    \"\"\"Detect if person is wearing a mask\"\"\"\n",
        "    try:\n",
        "        face = cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB)\n",
        "        face = cv2.resize(face, (224, 224))\n",
        "        face = img_to_array(face)\n",
        "        face = preprocess_input(face)\n",
        "        face = np.expand_dims(face, axis=0)\n",
        "\n",
        "        preds = mask_model.predict(face)[0]\n",
        "        max_idx = np.argmax(preds)\n",
        "        return MASK_LABELS[max_idx], float(preds[max_idx])\n",
        "    except Exception as e:\n",
        "        print(f\"Mask detection error: {str(e)}\")\n",
        "        return \"unknown\", 0.0\n",
        "\n",
        "def is_carried(obj_bbox, person_bbox):\n",
        "    \"\"\"Check if an object is being carried by a person\"\"\"\n",
        "    px1, py1, px2, py2 = person_bbox\n",
        "    ox1, oy1, ox2, oy2 = obj_bbox\n",
        "    obj_center_y = (oy1 + oy2) / 2\n",
        "    lower_half_threshold = py1 + (py2 - py1) * 0.6\n",
        "    overlap = (ox1 > px1) and (ox2 < px2) and (oy1 > py1) and (oy2 < py2)\n",
        "    in_carry_position = obj_center_y > lower_half_threshold\n",
        "    return overlap and in_carry_position\n",
        "\n",
        "def calculate_speed(prev_position, current_position, fps, frame_interval=1):\n",
        "    \"\"\"\n",
        "    Calculate speed in pixels per second between two positions\n",
        "    frame_interval: number of frames between measurements (for smoothing)\n",
        "    \"\"\"\n",
        "    if prev_position is None or current_position is None:\n",
        "        return 0\n",
        "\n",
        "    # Use center points for speed calculation\n",
        "    prev_center = ((prev_position[0] + prev_position[2]) / 2, (prev_position[1] + prev_position[3]) / 2)\n",
        "    curr_center = ((current_position[0] + current_position[2]) / 2, (current_position[1] + current_position[3]) / 2)\n",
        "\n",
        "    # Calculate Euclidean distance\n",
        "    distance = math.sqrt((curr_center[0] - prev_center[0])**2 + (curr_center[1] - prev_center[1])**2)\n",
        "\n",
        "    # Convert to pixels per second\n",
        "    speed = distance * fps / frame_interval\n",
        "    return speed\n",
        "\n",
        "# ===== Main Processing =====\n",
        "video_metadata = extract_video_metadata(SOURCE_VIDEO_PATH)\n",
        "video_info = sv.VideoInfo.from_video_path(SOURCE_VIDEO_PATH)\n",
        "\n",
        "# Get recording time from metadata or use current time as fallback\n",
        "try:\n",
        "    recording_time = datetime.strptime(video_metadata[\"creation_time\"].split(\".\")[0], \"%Y-%m-%dT%H:%M:%S\")\n",
        "    recording_time = recording_time.replace(tzinfo=timezone.utc)\n",
        "except (KeyError, ValueError):\n",
        "    print(\"Warning: Using current time as recording time fallback\")\n",
        "    recording_time = datetime.now(timezone.utc)\n",
        "\n",
        "with sv.VideoSink(TARGET_VIDEO_PATH, video_info) as sink:\n",
        "    for frame_number, result in enumerate(\n",
        "        model.track(source=SOURCE_VIDEO_PATH, tracker=\"bytetrack.yaml\", show=False, stream=True, persist=True)\n",
        "    ):\n",
        "        frame = result.orig_img\n",
        "        detections = sv.Detections.from_yolov8(result)\n",
        "\n",
        "        # Separate detections\n",
        "        people = []\n",
        "        objects = []\n",
        "        if result.boxes.id is not None:\n",
        "            tracker_ids = result.boxes.id.cpu().numpy().astype(int)\n",
        "            for i, (bbox, conf, class_id) in enumerate(zip(detections.xyxy, detections.confidence, detections.class_id)):\n",
        "                if class_id == 0:  # Person\n",
        "                    tracker_id = tracker_ids[i] if i < len(tracker_ids) else None\n",
        "                    people.append((bbox, conf, tracker_id))\n",
        "                elif class_id in BAG_CLASSES + [CAT_CLASS, DOG_CLASS]:\n",
        "                    objects.append((bbox, conf, class_id))\n",
        "\n",
        "        # Process each person\n",
        "        for person_bbox, confidence, tracker_id in people:\n",
        "            x1, y1, x2, y2 = person_bbox\n",
        "            bottom_right = (int(x2), int(y2))\n",
        "\n",
        "            # Calculate speed\n",
        "            current_position = (x1, y1, x2, y2)\n",
        "            prev_position = previous_positions.get(tracker_id, None)\n",
        "            speed = calculate_speed(prev_position, current_position, video_info.fps) if frame_number > 0 else 0\n",
        "            previous_positions[tracker_id] = current_position\n",
        "\n",
        "            # Age/gender detection\n",
        "            face_roi = frame[int(y1):int(y2), int(x1):int(x2)]\n",
        "            gender, age = predict_age_gender(face_roi)\n",
        "\n",
        "            # Mask detection\n",
        "            mask_status,mask_confidence = detect_mask(face_roi)  # Check if the person is wearing a mask\n",
        "\n",
        "            # Check for carried items\n",
        "            carried_items = []\n",
        "            for obj_bbox, _, class_id in objects:\n",
        "                if is_carried(obj_bbox, person_bbox):\n",
        "                    if class_id in BAG_CLASSES:\n",
        "                        carried_items.append(\"bag\")\n",
        "                    elif class_id == CAT_CLASS:\n",
        "                        carried_items.append(\"cat\")\n",
        "                    elif class_id == DOG_CLASS:\n",
        "                        carried_items.append(\"dog\")\n",
        "\n",
        "            # Initialize or update person data\n",
        "            if tracker_id not in tracker_states:\n",
        "                tracker_states[tracker_id] = []\n",
        "                detection_data[tracker_id] = {\n",
        "                    \"tracker_id\": int(tracker_id),\n",
        "                    \"gender\": gender,\n",
        "                    \"age\": age,\n",
        "                    \"carrying\": carried_items if carried_items else \"no objects\",\n",
        "                    \"mask_status\":mask_status,\n",
        "                    \"mask_confidence\":mask_confidence,\n",
        "                    \"bbox_history\": [],\n",
        "                    \"confidence\": float(confidence)\n",
        "                }\n",
        "\n",
        "            # Area crossing logic\n",
        "            in_area1 = cv2.pointPolygonTest(area1, bottom_right, False) >= 0\n",
        "            in_area2 = cv2.pointPolygonTest(area2, bottom_right, False) >= 0\n",
        "\n",
        "            if in_area2 and \"area2\" not in tracker_states[tracker_id]:\n",
        "                tracker_states[tracker_id].append(\"area2\")\n",
        "                if tracker_states[tracker_id] == [\"area2\"]:\n",
        "                    entry_time = recording_time + timedelta(seconds=frame_number / video_info.fps)\n",
        "                    detection_data[tracker_id][\"entry_time\"] = entry_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "                    detection_data[tracker_id][\"entry_frame\"] = frame_number\n",
        "                    # Mask detection\n",
        "                    mask_status,mask_confidence = detect_mask(face_roi)  # Check if the person is wearing a mask\n",
        "                    detection_data[tracker_id][\"entry_mask_status\"] = mask_status\n",
        "                    detection_data[tracker_id][\"entry_mask_confidence\"] = mask_confidence\n",
        "\n",
        "\n",
        "            if in_area1 and \"area1\" not in tracker_states[tracker_id]:\n",
        "                tracker_states[tracker_id].append(\"area1\")\n",
        "                if tracker_states[tracker_id] == [\"area1\"]:\n",
        "                    exit_time = recording_time + timedelta(seconds=frame_number / video_info.fps)\n",
        "                    detection_data[tracker_id][\"exit_time\"] = exit_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "                    detection_data[tracker_id][\"exit_frame\"] = frame_number\n",
        "                    # Mask detection\n",
        "                    mask_status,mask_confidence = detect_mask(face_roi)  # Check if the person is wearing a mask\n",
        "                    detection_data[tracker_id][\"exit_mask_status\"] = mask_status\n",
        "                    detection_data[tracker_id][\"exit_mask_confidence\"] = mask_confidence\n",
        "\n",
        "            # Update counts\n",
        "            if tracker_states[tracker_id] == [\"area2\", \"area1\"]:\n",
        "                entering_count += 1\n",
        "                tracker_states[tracker_id] = []\n",
        "            elif tracker_states[tracker_id] == [\"area1\", \"area2\"]:\n",
        "                exiting_count += 1\n",
        "                tracker_states[tracker_id] = []\n",
        "\n",
        "            total_count = entering_count + exiting_count\n",
        "\n",
        "            # Store bounding box history\n",
        "            detection_data[tracker_id][\"bbox_history\"].append({\n",
        "                \"frame_number\": int(frame_number),\n",
        "                \"bbox\": [float(x1), float(y1), float(x2), float(y2)],\n",
        "                \"timestamp\": (recording_time + timedelta(seconds=frame_number / video_info.fps)).strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                \"carrying\": carried_items if carried_items else \"no objects\",\n",
        "                \"speed\": speed,\n",
        "                \"mask_status\":mask_status,\n",
        "                \"mask_confidence\":mask_confidence\n",
        "            })\n",
        "\n",
        "            # Draw bounding box and label\n",
        "            carrying_str = \", \".join(carried_items) if carried_items else \"no objects\"\n",
        "            label = f\"ID: {tracker_id} | {gender}, {age} | Mask: {mask_status} | Carrying: {carrying_str} | Speed: {speed:.2f} px/s\"\n",
        "            cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 3)\n",
        "            cv2.putText(frame, label, (int(x1), int(y1) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
        "\n",
        "        # Draw counting areas and counters\n",
        "        cv2.polylines(frame, [area1], isClosed=True, color=(255, 0, 0), thickness=2)\n",
        "        cv2.polylines(frame, [area2], isClosed=True, color=(0, 255, 0), thickness=2)\n",
        "        cv2.putText(frame, f\"Total: {total_count}\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
        "        cv2.putText(frame, f\"Entering: {entering_count}\", (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "        cv2.putText(frame, f\"Exiting: {exiting_count}\", (50, 150), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
        "\n",
        "        # Write frame to output video\n",
        "        sink.write_frame(frame)\n",
        "\n",
        "# ===== Save Results =====\n",
        "json_output = {\n",
        "    \"video_metadata\": video_metadata,\n",
        "    \"processing_time\": datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S %Z\"),\n",
        "    \"summary\": {\n",
        "        \"total_people\": int(total_count),\n",
        "        \"total_entering\": int(entering_count),\n",
        "        \"total_exiting\": int(exiting_count),\n",
        "        \"fps\": float(video_info.fps),\n",
        "        \"duration_seconds\": float(video_info.total_frames / video_info.fps)\n",
        "    },\n",
        "    \"detections\": {\n",
        "        int(tracker_id): data for tracker_id, data in detection_data.items()\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(JSON_OUTPUT_PATH, \"w\") as f:\n",
        "    json.dump(json_output, f, indent=4)\n",
        "\n",
        "print(f\"Processing complete. Results saved to {TARGET_VIDEO_PATH} and {JSON_OUTPUT_PATH}\")\n",
        "print(f\"Total: {total_count}, Entering: {entering_count}, Exiting: {exiting_count}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Restricted Area People Entering Monitoring \n",
        "\n",
        "##### Enter Exit Count + Age Gender + Video Metadata + Carrying Object + Face Mask Detection + Speed Calculation + Restricted Area\n",
        "\n",
        "* Add people entering detection and counting for restricted area to above code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import supervision as sv\n",
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "import numpy as np\n",
        "import json\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "from deepface import DeepFace\n",
        "import ffmpeg\n",
        "from datetime import timezone\n",
        "import math\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "\n",
        "\n",
        "# ===== Configuration =====\n",
        "TARGET_VIDEO_PATH = \"output_video_restrcted.mp4\"\n",
        "JSON_OUTPUT_PATH = \"detection_data_restrcted.json\"\n",
        "\n",
        "# Define polygonal areas for counting\n",
        "area1 = np.array([(1169, 1678+50), (1942, 2025+50), (1816, 2102+50), (1085, 1703+50)], np.int32)\n",
        "area2 = np.array([(1040, 1710+50), (1771, 2117+50), (1673, 2142+50), (981, 1713+50)], np.int32)\n",
        "\n",
        "# Add a restricted area (randomly chosen coordinates - adjust as needed)\n",
        "restricted_area = np.array([(500, 500), (800, 500), (800, 800), (500, 800)], np.int32)\n",
        "\n",
        "# Object classes (COCO dataset)\n",
        "BAG_CLASSES = [24, 26, 28]  # backpack, handbag, suitcase\n",
        "CAT_CLASS = 15\n",
        "DOG_CLASS = 16\n",
        "\n",
        "# Mask detection labels\n",
        "MASK_LABELS = [\"mask\", \"no_mask\", \"improper\"]\n",
        "\n",
        "# ===== Initialize =====\n",
        "total_count = 0\n",
        "entering_count = 0\n",
        "exiting_count = 0\n",
        "restricted_area_count = 0  # New counter for restricted area\n",
        "tracker_states = {}\n",
        "restricted_area_states = {}  # To track who entered restricted area\n",
        "detection_data = {}\n",
        "previous_positions = {}  # To track movement between frames\n",
        "\n",
        "# ===== Helper Functions =====\n",
        "def extract_video_metadata(video_path):\n",
        "    \"\"\"Extract all available metadata from a video file using ffmpeg-python.\"\"\"\n",
        "    try:\n",
        "        probe = ffmpeg.probe(video_path)\n",
        "        metadata = {}\n",
        "\n",
        "        # ===== 1. General Video Information =====\n",
        "        if \"format\" in probe:\n",
        "            format_info = probe[\"format\"]\n",
        "            metadata.update({\n",
        "                \"filename\": format_info.get(\"filename\"),\n",
        "                \"format_name\": format_info.get(\"format_name\"),\n",
        "                \"format_long_name\": format_info.get(\"format_long_name\"),\n",
        "                \"duration_seconds\": float(format_info.get(\"duration\", 0)),\n",
        "                \"size_bytes\": int(format_info.get(\"size\", 0)),\n",
        "                \"bitrate\": int(format_info.get(\"bit_rate\", 0)),\n",
        "            })\n",
        "\n",
        "            # Extract creation_time (if available)\n",
        "            if \"tags\" in format_info:\n",
        "                metadata.update({\n",
        "                    \"creation_time\": format_info[\"tags\"].get(\"creation_time\"),\n",
        "                    \"encoder\": format_info[\"tags\"].get(\"encoder\"),\n",
        "                })\n",
        "\n",
        "        # ===== 2. Video Stream Metadata =====\n",
        "        video_streams = [s for s in probe[\"streams\"] if s[\"codec_type\"] == \"video\"]\n",
        "        if video_streams:\n",
        "            video_info = video_streams[0]\n",
        "            metadata.update({\n",
        "                \"video_codec\": video_info.get(\"codec_name\"),\n",
        "                \"width\": int(video_info.get(\"width\", 0)),\n",
        "                \"height\": int(video_info.get(\"height\", 0)),\n",
        "                \"fps\": eval(video_info.get(\"avg_frame_rate\", \"0/1\")),  # e.g., \"30/1\" â†’ 30.0\n",
        "            })\n",
        "\n",
        "            # Extract device-specific metadata (iPhone, Android, etc.)\n",
        "            if \"tags\" in video_info:\n",
        "                metadata.update({\n",
        "                    \"device_model\": video_info[\"tags\"].get(\"com.apple.quicktime.model\"),\n",
        "                    \"software\": video_info[\"tags\"].get(\"software\"),\n",
        "                })\n",
        "\n",
        "        # ===== 3. Audio Stream Metadata =====\n",
        "        audio_streams = [s for s in probe[\"streams\"] if s[\"codec_type\"] == \"audio\"]\n",
        "        if audio_streams:\n",
        "            audio_info = audio_streams[0]\n",
        "            metadata.update({\n",
        "                \"audio_codec\": audio_info.get(\"codec_name\"),\n",
        "                \"sample_rate\": int(audio_info.get(\"sample_rate\", 0)),\n",
        "                \"channels\": int(audio_info.get(\"channels\", 0)),\n",
        "            })\n",
        "\n",
        "        # ===== 4. GPS Coordinates (if recorded) =====\n",
        "        if \"format\" in probe and \"tags\" in probe[\"format\"]:\n",
        "            tags = probe[\"format\"][\"tags\"]\n",
        "            if \"location\" in tags:  # Some Android devices store GPS here\n",
        "                metadata[\"gps_coordinates\"] = tags[\"location\"]\n",
        "            elif \"com.apple.quicktime.location.ISO6709\" in tags:  # iPhone GPS\n",
        "                metadata[\"gps_coordinates\"] = tags[\"com.apple.quicktime.location.ISO6709\"]\n",
        "\n",
        "        # ===== 5. Convert ISO Timestamp to Readable Format =====\n",
        "        if \"creation_time\" in metadata:\n",
        "            try:\n",
        "                dt = datetime.strptime(metadata[\"creation_time\"].split(\".\")[0], \"%Y-%m-%dT%H:%M:%S\")\n",
        "                dt = dt.replace(tzinfo=timezone.utc)\n",
        "                metadata[\"creation_time_utc\"] = dt.strftime(\"%Y-%m-%d %H:%M:%S UTC\")\n",
        "                metadata[\"creation_time_local\"] = dt.astimezone().strftime(\"%Y-%m-%d %H:%M:%S %Z\")\n",
        "                metadata[\"recording_time\"] = metadata[\"creation_time_local\"]  # For backward compatibility\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        return metadata\n",
        "\n",
        "    except ffmpeg.Error as e:\n",
        "        print(f\"FFmpeg error: {e.stderr.decode('utf-8')}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def predict_age_gender(face_img):\n",
        "    \"\"\"Predict age and gender using DeepFace\"\"\"\n",
        "    try:\n",
        "        analysis = DeepFace.analyze(face_img, actions=[\"age\", \"gender\"], enforce_detection=False)\n",
        "        return analysis[0][\"dominant_gender\"], analysis[0][\"age\"]\n",
        "    except Exception:\n",
        "        return \"Unknown\", \"Unknown\"\n",
        "\n",
        "def detect_mask(face_img):\n",
        "    \"\"\"Detect if person is wearing a mask\"\"\"\n",
        "    try:\n",
        "        face = cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB)\n",
        "        face = cv2.resize(face, (224, 224))\n",
        "        face = img_to_array(face)\n",
        "        face = preprocess_input(face)\n",
        "        face = np.expand_dims(face, axis=0)\n",
        "\n",
        "        preds = mask_model.predict(face)[0]\n",
        "        max_idx = np.argmax(preds)\n",
        "        return MASK_LABELS[max_idx], float(preds[max_idx])\n",
        "    except Exception as e:\n",
        "        print(f\"Mask detection error: {str(e)}\")\n",
        "        return \"unknown\", 0.0\n",
        "\n",
        "def is_carried(obj_bbox, person_bbox):\n",
        "    \"\"\"Check if an object is being carried by a person\"\"\"\n",
        "    px1, py1, px2, py2 = person_bbox\n",
        "    ox1, oy1, ox2, oy2 = obj_bbox\n",
        "    obj_center_y = (oy1 + oy2) / 2\n",
        "    lower_half_threshold = py1 + (py2 - py1) * 0.6\n",
        "    overlap = (ox1 > px1) and (ox2 < px2) and (oy1 > py1) and (oy2 < py2)\n",
        "    in_carry_position = obj_center_y > lower_half_threshold\n",
        "    return overlap and in_carry_position\n",
        "\n",
        "def calculate_speed(prev_position, current_position, fps, frame_interval=1):\n",
        "    \"\"\"\n",
        "    Calculate speed in pixels per second between two positions\n",
        "    frame_interval: number of frames between measurements (for smoothing)\n",
        "    \"\"\"\n",
        "    if prev_position is None or current_position is None:\n",
        "        return 0\n",
        "\n",
        "    # Use center points for speed calculation\n",
        "    prev_center = ((prev_position[0] + prev_position[2]) / 2, (prev_position[1] + prev_position[3]) / 2)\n",
        "    curr_center = ((current_position[0] + current_position[2]) / 2, (current_position[1] + current_position[3]) / 2)\n",
        "\n",
        "    # Calculate Euclidean distance\n",
        "    distance = math.sqrt((curr_center[0] - prev_center[0])**2 + (curr_center[1] - prev_center[1])**2)\n",
        "\n",
        "    # Convert to pixels per second\n",
        "    speed = distance * fps / frame_interval\n",
        "    return speed\n",
        "\n",
        "# ===== Main Processing =====\n",
        "video_metadata = extract_video_metadata(SOURCE_VIDEO_PATH)\n",
        "video_info = sv.VideoInfo.from_video_path(SOURCE_VIDEO_PATH)\n",
        "\n",
        "# Get recording time from metadata or use current time as fallback\n",
        "try:\n",
        "    recording_time = datetime.strptime(video_metadata[\"creation_time\"].split(\".\")[0], \"%Y-%m-%dT%H:%M:%S\")\n",
        "    recording_time = recording_time.replace(tzinfo=timezone.utc)\n",
        "except (KeyError, ValueError):\n",
        "    print(\"Warning: Using current time as recording time fallback\")\n",
        "    recording_time = datetime.now(timezone.utc)\n",
        "\n",
        "with sv.VideoSink(TARGET_VIDEO_PATH, video_info) as sink:\n",
        "    for frame_number, result in enumerate(\n",
        "        model.track(source=SOURCE_VIDEO_PATH, tracker=\"bytetrack.yaml\", show=False, stream=True, persist=True)\n",
        "    ):\n",
        "        frame = result.orig_img\n",
        "        detections = sv.Detections.from_yolov8(result)\n",
        "\n",
        "        # Separate detections\n",
        "        people = []\n",
        "        objects = []\n",
        "        if result.boxes.id is not None:\n",
        "            tracker_ids = result.boxes.id.cpu().numpy().astype(int)\n",
        "            for i, (bbox, conf, class_id) in enumerate(zip(detections.xyxy, detections.confidence, detections.class_id)):\n",
        "                if class_id == 0:  # Person\n",
        "                    tracker_id = tracker_ids[i] if i < len(tracker_ids) else None\n",
        "                    people.append((bbox, conf, tracker_id))\n",
        "                elif class_id in BAG_CLASSES + [CAT_CLASS, DOG_CLASS]:\n",
        "                    objects.append((bbox, conf, class_id))\n",
        "\n",
        "        # Process each person\n",
        "        for person_bbox, confidence, tracker_id in people:\n",
        "            x1, y1, x2, y2 = person_bbox\n",
        "            bottom_right = (int(x2), int(y2))\n",
        "            center = (int((x1 + x2) / 2), int((y1 + y2) / 2))\n",
        "\n",
        "            # Calculate speed\n",
        "            current_position = (x1, y1, x2, y2)\n",
        "            prev_position = previous_positions.get(tracker_id, None)\n",
        "            speed = calculate_speed(prev_position, current_position, video_info.fps) if frame_number > 0 else 0\n",
        "            previous_positions[tracker_id] = current_position\n",
        "\n",
        "            # Age/gender detection\n",
        "            face_roi = frame[int(y1):int(y2), int(x1):int(x2)]\n",
        "            gender, age = predict_age_gender(face_roi)\n",
        "\n",
        "            # Mask detection\n",
        "            mask_status, mask_confidence = detect_mask(face_roi)\n",
        "\n",
        "            # Check for carried items\n",
        "            carried_items = []\n",
        "            for obj_bbox, _, class_id in objects:\n",
        "                if is_carried(obj_bbox, person_bbox):\n",
        "                    if class_id in BAG_CLASSES:\n",
        "                        carried_items.append(\"bag\")\n",
        "                    elif class_id == CAT_CLASS:\n",
        "                        carried_items.append(\"cat\")\n",
        "                    elif class_id == DOG_CLASS:\n",
        "                        carried_items.append(\"dog\")\n",
        "\n",
        "            # Initialize or update person data\n",
        "            if tracker_id not in tracker_states:\n",
        "                tracker_states[tracker_id] = []\n",
        "                restricted_area_states[tracker_id] = False\n",
        "                detection_data[tracker_id] = {\n",
        "                    \"tracker_id\": int(tracker_id),\n",
        "                    \"gender\": gender,\n",
        "                    \"age\": age,\n",
        "                    \"carrying\": carried_items if carried_items else \"no objects\",\n",
        "                    \"mask_status\": mask_status,\n",
        "                    \"mask_confidence\": mask_confidence,\n",
        "                    \"bbox_history\": [],\n",
        "                    \"confidence\": float(confidence),\n",
        "                    \"entered_restricted_area\": False,\n",
        "                    \"restricted_area_entry_time\": None,\n",
        "                    \"restricted_area_entry_frame\": None\n",
        "                }\n",
        "\n",
        "            # Check restricted area entry\n",
        "            in_restricted_area = cv2.pointPolygonTest(restricted_area, center, False) >= 0\n",
        "            if in_restricted_area and not restricted_area_states[tracker_id]:\n",
        "                restricted_area_states[tracker_id] = True\n",
        "                detection_data[tracker_id][\"entered_restricted_area\"] = True\n",
        "                detection_data[tracker_id][\"restricted_area_entry_time\"] = (\n",
        "                    recording_time + timedelta(seconds=frame_number / video_info.fps)).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "                detection_data[tracker_id][\"restricted_area_entry_frame\"] = frame_number\n",
        "                restricted_area_count += 1\n",
        "            elif not in_restricted_area and restricted_area_states[tracker_id]:\n",
        "                restricted_area_states[tracker_id] = False\n",
        "\n",
        "            # Area crossing logic\n",
        "            in_area1 = cv2.pointPolygonTest(area1, bottom_right, False) >= 0\n",
        "            in_area2 = cv2.pointPolygonTest(area2, bottom_right, False) >= 0\n",
        "\n",
        "            if in_area2 and \"area2\" not in tracker_states[tracker_id]:\n",
        "                tracker_states[tracker_id].append(\"area2\")\n",
        "                if tracker_states[tracker_id] == [\"area2\"]:\n",
        "                    entry_time = recording_time + timedelta(seconds=frame_number / video_info.fps)\n",
        "                    detection_data[tracker_id][\"entry_time\"] = entry_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "                    detection_data[tracker_id][\"entry_frame\"] = frame_number\n",
        "                    detection_data[tracker_id][\"entry_mask_status\"] = mask_status\n",
        "                    detection_data[tracker_id][\"entry_mask_confidence\"] = mask_confidence\n",
        "\n",
        "            if in_area1 and \"area1\" not in tracker_states[tracker_id]:\n",
        "                tracker_states[tracker_id].append(\"area1\")\n",
        "                if tracker_states[tracker_id] == [\"area1\"]:\n",
        "                    exit_time = recording_time + timedelta(seconds=frame_number / video_info.fps)\n",
        "                    detection_data[tracker_id][\"exit_time\"] = exit_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "                    detection_data[tracker_id][\"exit_frame\"] = frame_number\n",
        "                    detection_data[tracker_id][\"exit_mask_status\"] = mask_status\n",
        "                    detection_data[tracker_id][\"exit_mask_confidence\"] = mask_confidence\n",
        "\n",
        "            # Update counts\n",
        "            if tracker_states[tracker_id] == [\"area2\", \"area1\"]:\n",
        "                entering_count += 1\n",
        "                tracker_states[tracker_id] = []\n",
        "            elif tracker_states[tracker_id] == [\"area1\", \"area2\"]:\n",
        "                exiting_count += 1\n",
        "                tracker_states[tracker_id] = []\n",
        "\n",
        "            total_count = entering_count + exiting_count\n",
        "\n",
        "            # Store bounding box history\n",
        "            detection_data[tracker_id][\"bbox_history\"].append({\n",
        "                \"frame_number\": int(frame_number),\n",
        "                \"bbox\": [float(x1), float(y1), float(x2), float(y2)],\n",
        "                \"timestamp\": (recording_time + timedelta(seconds=frame_number / video_info.fps)).strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                \"carrying\": carried_items if carried_items else \"no objects\",\n",
        "                \"speed\": speed,\n",
        "                \"mask_status\": mask_status,\n",
        "                \"mask_confidence\": mask_confidence,\n",
        "                \"in_restricted_area\": in_restricted_area\n",
        "            })\n",
        "\n",
        "            # Draw bounding box and label\n",
        "            carrying_str = \", \".join(carried_items) if carried_items else \"no objects\"\n",
        "            label = f\"ID: {tracker_id} | {gender}, {age} | Mask: {mask_status} | Carrying: {carrying_str} | Speed: {speed:.2f} px/s\"\n",
        "            if in_restricted_area:\n",
        "                label += \" | IN RESTRICTED AREA\"\n",
        "                box_color = (0, 0, 255)  # Red for restricted area\n",
        "            else:\n",
        "                box_color = (255, 0, 0)  # Blue for normal\n",
        "\n",
        "            cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), box_color, 3)\n",
        "            cv2.putText(frame, label, (int(x1), int(y1) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
        "\n",
        "        # Draw counting areas and counters\n",
        "        cv2.polylines(frame, [area1], isClosed=True, color=(255, 0, 0), thickness=2)\n",
        "        cv2.polylines(frame, [area2], isClosed=True, color=(0, 255, 0), thickness=2)\n",
        "        cv2.polylines(frame, [restricted_area], isClosed=True, color=(0, 0, 255), thickness=2)\n",
        "        cv2.putText(frame, \"Restricted Area\", (restricted_area[0][0], restricted_area[0][1] - 10),\n",
        "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
        "\n",
        "        # Display counters\n",
        "        cv2.putText(frame, f\"Total: {total_count}\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
        "        cv2.putText(frame, f\"Entering: {entering_count}\", (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "        cv2.putText(frame, f\"Exiting: {exiting_count}\", (50, 150), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
        "        cv2.putText(frame, f\"Restricted Area Entries: {restricted_area_count}\", (50, 200),\n",
        "                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 165, 255), 2)\n",
        "\n",
        "        # Write frame to output video\n",
        "        sink.write_frame(frame)\n",
        "\n",
        "# ===== Save Results =====\n",
        "json_output = {\n",
        "    \"video_metadata\": video_metadata,\n",
        "    \"processing_time\": datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S %Z\"),\n",
        "    \"summary\": {\n",
        "        \"total_people\": int(total_count),\n",
        "        \"total_entering\": int(entering_count),\n",
        "        \"total_exiting\": int(exiting_count),\n",
        "        \"restricted_area_entries\": int(restricted_area_count),  # New field\n",
        "        \"fps\": float(video_info.fps),\n",
        "        \"duration_seconds\": float(video_info.total_frames / video_info.fps)\n",
        "    },\n",
        "    \"detections\": {\n",
        "        int(tracker_id): data for tracker_id, data in detection_data.items()\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(JSON_OUTPUT_PATH, \"w\") as f:\n",
        "    json.dump(json_output, f, indent=4)\n",
        "\n",
        "print(f\"Processing complete. Results saved to {TARGET_VIDEO_PATH} and {JSON_OUTPUT_PATH}\")\n",
        "print(f\"Total: {total_count}, Entering: {entering_count}, Exiting: {exiting_count}\")\n",
        "print(f\"Restricted Area Entries: {restricted_area_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### FRame Bases detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import supervision as sv\n",
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "import numpy as np\n",
        "import json\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "from deepface import DeepFace\n",
        "import ffmpeg\n",
        "from datetime import timezone\n",
        "import math\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "\n",
        "# ===== Configuration =====\n",
        "TARGET_VIDEO_PATH = \"output_video_final_trackingID_based.mp4\"\n",
        "JSON_OUTPUT_PATH = \"detection_data_final_trackingID_based.json\"\n",
        "\n",
        "# Load models (add these at the beginning)\n",
        "model = YOLO(\"yolov8n.pt\")  # or whatever model you're using\n",
        "\n",
        "# Define polygonal areas for counting\n",
        "area1 = np.array([(1169, 1678+50), (1942, 2025+50), (1816, 2102+50), (1085, 1703+50)], np.int32)\n",
        "area2 = np.array([(1040, 1710+50), (1771, 2117+50), (1673, 2142+50), (981, 1713+50)], np.int32)\n",
        "\n",
        "# Define restricted area (example coordinates - adjust as needed)\n",
        "restricted_area = np.array([(500, 500), (800, 500), (800, 800), (500, 800)], np.int32)\n",
        "\n",
        "# Object classes (COCO dataset)\n",
        "BAG_CLASSES = [24, 26, 28]  # backpack, handbag, suitcase\n",
        "CAT_CLASS = 15\n",
        "DOG_CLASS = 16\n",
        "\n",
        "# Mask detection labels\n",
        "MASK_LABELS = [\"mask\", \"no_mask\", \"improper\"]\n",
        "\n",
        "# ===== Initialize =====\n",
        "total_count = 0\n",
        "entering_count = 0\n",
        "exiting_count = 0\n",
        "restricted_area_count = 0\n",
        "tracker_history = {}  # To maintain state across frames\n",
        "restricted_people = set()  # Track people who entered restricted area\n",
        "\n",
        "# [Rest of your helper functions remain the same...]\n",
        "\n",
        "# ===== Main Processing =====\n",
        "video_metadata = extract_video_metadata(SOURCE_VIDEO_PATH)\n",
        "video_info = sv.VideoInfo.from_video_path(SOURCE_VIDEO_PATH)\n",
        "\n",
        "# Get recording time from metadata or use current time as fallback\n",
        "try:\n",
        "    recording_time = datetime.strptime(video_metadata[\"creation_time\"].split(\".\")[0], \"%Y-%m-%dT%H:%M:%S\")\n",
        "    recording_time = recording_time.replace(tzinfo=timezone.utc)\n",
        "except (KeyError, ValueError):\n",
        "    print(\"Warning: Using current time as recording time fallback\")\n",
        "    recording_time = datetime.now(timezone.utc)\n",
        "\n",
        "frame_detections = []  # This will store all frame-based detections\n",
        "\n",
        "with sv.VideoSink(TARGET_VIDEO_PATH, video_info) as sink:\n",
        "    for frame_number, result in enumerate(\n",
        "        model.track(source=SOURCE_VIDEO_PATH, tracker=\"bytetrack.yaml\", show=False, stream=True, persist=True)\n",
        "    ):\n",
        "        frame = result.orig_img\n",
        "        detections = sv.Detections.from_yolov8(result)\n",
        "        \n",
        "        current_frame_data = {\n",
        "            \"frame_number\": frame_number,\n",
        "            \"timestamp\": (recording_time + timedelta(seconds=frame_number / video_info.fps)).strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "            \"detections\": []\n",
        "        }\n",
        "\n",
        "        # Initialize objects and people lists for this frame\n",
        "        objects = []\n",
        "        people = []\n",
        "        \n",
        "        if result.boxes.id is not None:\n",
        "            tracker_ids = result.boxes.id.cpu().numpy().astype(int)\n",
        "            for i, (bbox, conf, class_id) in enumerate(zip(detections.xyxy, detections.confidence, detections.class_id)):\n",
        "                if class_id == 0:  # Person\n",
        "                    tracker_id = tracker_ids[i] if i < len(tracker_ids) else None\n",
        "                    if tracker_id is not None:\n",
        "                        people.append((bbox, conf, tracker_id))\n",
        "                elif class_id in BAG_CLASSES + [CAT_CLASS, DOG_CLASS]:  # Objects we care about\n",
        "                    objects.append((bbox, conf, class_id))\n",
        "\n",
        "        # Process each person in the current frame\n",
        "        for bbox, conf, tracker_id in people:\n",
        "            x1, y1, x2, y2 = bbox\n",
        "            bottom_center = (int((x1+x2)/2), int(y2))\n",
        "\n",
        "            # Check area crossings\n",
        "            in_area1 = cv2.pointPolygonTest(area1, bottom_center, False) >= 0\n",
        "            in_area2 = cv2.pointPolygonTest(area2, bottom_center, False) >= 0\n",
        "            in_restricted = cv2.pointPolygonTest(restricted_area, bottom_center, False) >= 0\n",
        "\n",
        "            # Initialize tracker history if new person\n",
        "            if tracker_id not in tracker_history:\n",
        "                tracker_history[tracker_id] = {\n",
        "                    'first_seen': frame_number,\n",
        "                    'last_seen': frame_number,\n",
        "                    'entered_restricted': False,\n",
        "                    'entry_frame': None,\n",
        "                    'exit_frame': None,\n",
        "                    'entry_time': None,\n",
        "                    'exit_time': None,\n",
        "                    'gender': \"Unknown\",\n",
        "                    'age': \"Unknown\",\n",
        "                    'carrying': \"none\",\n",
        "                    'mask_status': \"unknown\",\n",
        "                    'mask_confidence': 0.0\n",
        "                }\n",
        "            else:\n",
        "                tracker_history[tracker_id]['last_seen'] = frame_number\n",
        "\n",
        "            # Update restricted area status\n",
        "            if in_restricted and not tracker_history[tracker_id]['entered_restricted']:\n",
        "                tracker_history[tracker_id]['entered_restricted'] = True\n",
        "                restricted_area_count += 1\n",
        "                restricted_people.add(tracker_id)\n",
        "\n",
        "            # Analyze person attributes if entering monitored area\n",
        "            if (in_area1 or in_area2) and tracker_history[tracker_id]['entry_frame'] is None:\n",
        "                gender, age, mask_status, mask_conf, carrying = analyze_person(frame, bbox, objects)\n",
        "                entry_time = recording_time + timedelta(seconds=frame_number / video_info.fps)\n",
        "                \n",
        "                tracker_history[tracker_id].update({\n",
        "                    'entry_frame': frame_number,\n",
        "                    'entry_time': entry_time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                    'gender': gender,\n",
        "                    'age': age,\n",
        "                    'carrying': carrying,\n",
        "                    'mask_status': mask_status,\n",
        "                    'mask_confidence': mask_conf\n",
        "                })\n",
        "                entering_count += 1\n",
        "                total_count += 1\n",
        "\n",
        "            # Create detection entry for current frame\n",
        "            detection_entry = {\n",
        "                \"tracker_id\": int(tracker_id),\n",
        "                \"class_id\": 0,  # 0 is for person in COCO\n",
        "                \"class_name\": \"person\",\n",
        "                \"confidence\": float(conf),\n",
        "                \"bbox\": [float(x1), float(y1), float(x2), float(y2)],\n",
        "                \"in_area1\": in_area1,\n",
        "                \"in_area2\": in_area2,\n",
        "                \"in_restricted_area\": in_restricted,\n",
        "                \"gender\": tracker_history[tracker_id]['gender'],\n",
        "                \"age\": tracker_history[tracker_id]['age'],\n",
        "                \"carrying\": tracker_history[tracker_id]['carrying'],\n",
        "                \"mask_status\": tracker_history[tracker_id]['mask_status'],\n",
        "                \"mask_confidence\": tracker_history[tracker_id]['mask_confidence'],\n",
        "                \"entry_time\": tracker_history[tracker_id]['entry_time'],\n",
        "                \"exit_time\": tracker_history[tracker_id]['exit_time'],\n",
        "                \"first_seen_frame\": tracker_history[tracker_id]['first_seen'],\n",
        "                \"last_seen_frame\": tracker_history[tracker_id]['last_seen'],\n",
        "                \"entered_restricted\": tracker_history[tracker_id]['entered_restricted']\n",
        "            }\n",
        "\n",
        "            current_frame_data['detections'].append(detection_entry)\n",
        "\n",
        "            # Draw visualizations\n",
        "            if tracker_history[tracker_id]['entry_time']:\n",
        "                label = f\"ID: {tracker_id} | {tracker_history[tracker_id]['gender']}, {tracker_history[tracker_id]['age']} | Mask: {tracker_history[tracker_id]['mask_status']}\"\n",
        "                if tracker_history[tracker_id]['entered_restricted']:\n",
        "                    cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 0, 255), 3)\n",
        "                    label += \" | RESTRICTED\"\n",
        "                else:\n",
        "                    cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
        "                cv2.putText(frame, label, (int(x1), int(y1)-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 1)\n",
        "\n",
        "        # Add frame data to our collection\n",
        "        frame_detections.append(current_frame_data)\n",
        "\n",
        "        sink.write_frame(frame)\n",
        "\n",
        "\n",
        "# Update exit times for people who were tracked\n",
        "for tracker_id, data in tracker_history.items():\n",
        "    if data['entry_time'] and not data['exit_time']:\n",
        "        exit_time = recording_time + timedelta(seconds=data['last_seen'] / video_info.fps)\n",
        "        data['exit_time'] = exit_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        exiting_count += 1\n",
        "\n",
        "# ===== Save Results =====\n",
        "json_output = {\n",
        "    \"video_metadata\": video_metadata,\n",
        "    \"processing_time\": datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S %Z\"),\n",
        "    \"summary\": {\n",
        "        \"total_people\": int(total_count),\n",
        "        \"total_entering\": int(entering_count),\n",
        "        \"total_exiting\": int(exiting_count),\n",
        "        \"restricted_area_entries\": int(restricted_area_count),\n",
        "        \"restricted_people_ids\": [int(id) for id in restricted_people],\n",
        "        \"fps\": float(video_info.fps),\n",
        "        \"duration_seconds\": float(video_info.total_frames / video_info.fps)\n",
        "    },\n",
        "    \"frame_detections\": frame_detections\n",
        "}\n",
        "\n",
        "with open(JSON_OUTPUT_PATH, \"w\") as f:\n",
        "    json.dump(json_output, f, indent=4)\n",
        "\n",
        "print(f\"Processing complete. Results saved to {TARGET_VIDEO_PATH} and {JSON_OUTPUT_PATH}\")\n",
        "print(f\"Total: {total_count}, Entering: {entering_count}, Exiting: {exiting_count}\")\n",
        "print(f\"Restricted area entries: {restricted_area_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Download Output Video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxKTLTbVyeTB"
      },
      "outputs": [],
      "source": [
        "from IPython.display import FileLink\n",
        "FileLink(r'output_video.mp4')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Nu9nl9GiyeS-",
        "U_AKZ0YNN78S",
        "sJ_UfDWfyeTA"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 6929079,
          "sourceId": 11113447,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30919,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
